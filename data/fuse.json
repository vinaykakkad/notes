{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Notes 📚","n":0.707},"1":{"v":"\n![home](/assets/images/home.png)\n","n":1}}},{"i":2,"$":{"0":{"v":"Dev","n":1}}},{"i":3,"$":{"0":{"v":"JS","n":1}}},{"i":4,"$":{"0":{"v":"Var Let Const","n":0.577},"1":{"v":"\n# Lexical environment and scope\n\n- memory space of the current execution context + LE of parent\n- recursive expansion (LE of parent -> LE of parent of parent ) is `scope chain`\n  - recursively we reach the global\n  - ***parent of global is none****\n  - ***LE of some parent function is stored as closure***\n\n## Scope\n\n- part of which can access a variable is scope of variable\n\n# Shortest JS code\n\n- even when we don't have anything in our code, JS engines creates a global object\n  - window in case of browsers\n  - and in the global context `this` variables refers to the global object\n\n![](/assets/images/2022-10-17-20-30-44.png)\n\n# Let and Const\n\n- memory is assigned before execution to every variable(var, let, const)\n- but in case of `let` and `const` memory is not allocated in the global space\n  - it is allocated in something, that is known as the `temporal dead zone`\n  - ***therefore we cannot access let and const variable without initialization***\n  - ***it will give us an reference error***\n  - return values for:\n    - var without initialization: `undefined`\n    - let, const without initialization: `reference error`\n    - any variable without declaration: `reference error`\n\n## Re-declaration vs re-initialization\n\n- only var can be re-declared in the same scope\n- var and let can be re-initialized, const cannot be\n- const cannot be declared without initialization\n\n## Errors\n\n- syntax error: when we re-declare some variable\n  - or when we declare const without initialization\n- type error: when we re-initialize cost\n- reference error: when we access something that is not in the current memory space (TDZ or un-declared)\n\n# Block Scope\n\n- block or compound statement\n  - group of statement surrounded within curly braces\n- block scoped variables\n  - `let` and `const` are block scoped variables\n    - i.e. if they are defined within a block, they won't be accessible outside it\n      - ***will throw reference error(not defined)***\n  \n<blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">- let and const are block scoped => their memory allocation will happen in the nearest block memory space<br>- var is function scoped=> its memory allocation will happen in the nearest function memory space</blockquote>\n\n## Shadowing\n\n- shadowing is when a globally declared variable is used / declared inside a block scope\n  - for `var`\n    - as it is not block scope, the new variable refers to the same memory address\n    - **_`var` is function scope_**\n  - for `let` and `const`\n    - the are block scoped variables\n    - memory is allocated in a separate memory space (block)\n\n### Illegal Shadowing\n\n- when a `let` or `const` variable is declared in global scope and we try to declare a `var` variable inside a block\n  - it will throw an syntax error due to `illegal-shadowing`\n\n![](/assets/images/2023-10-03-17-13-44.png)","n":0.048}}},{"i":5,"$":{"0":{"v":"How JS Works","n":0.577},"1":{"v":"\n- everything in JS happens inside an `execution context`\n\n# Execution Context\n\n![](/assets/images/2022-10-09-18-59-44.png)\n\n- has two main components\n  - `variable environment` or `memory-space` for storing key value pairs of variables and functions\n  - `thread of execution` or `code-component` where code is executed\n\n# How Code is executed\n\n- code is executed in two phases\n  - *phase 1*: memory allocation\n    - key value pairs for all variables and functions are stored\n      - variables are stored as undefined\n      - for functions the entire code is stored in memory\n  - *phase 2*: execution\n    - every context is executed line by line\n    - when a function is called, a new context(in call stack) is created and entire process is repeated\n    - when the function returns, the new context is deleted and control is returned back to global context\n\n## Call Stack\n\n- global context always at the bottom\n- others are pushed at the time of functions\n- contexts are popped when control is returned\n","n":0.081}}},{"i":6,"$":{"0":{"v":"Hoisting","n":1},"1":{"v":"# Hoisting in JS\n\n- accessing variables and functions before initialization\n\n## Why this happens\n\n- before the actual execution\n  - memory is allocated to every variable and function\n  - thus we can access variables before initialization\n\n## Cases\n\n- accessing variable: return `undefined`\n- accessing a named function: returns `the code of the function`\n- calling the function: executes the function without any errors\n- accessing anonymous function: returns `undefined` (is ultimately an variable)\n- accessing undeclared variable: returns `not-defined` error\n","n":0.117}}},{"i":7,"$":{"0":{"v":"CLI","n":1}}},{"i":8,"$":{"0":{"v":"Shell Scripting","n":0.707},"1":{"v":"\n## Shell vs Terminal\n\n- kernel - server\n- shell - middleware\n- terminal - client\n\n## shell prompt\n\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">$</code> at the end of the prompt ⇒ normal user\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">#</code> at the end of the prompt ⇒ root user\n    - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">sudo su</code> to open a terminal as a root user\n\n## shell functions\n\n- they are built-in programs stored on our file system\n- the shell searches for these programs through the <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">PATH</code> environment variable\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">which</code> gives the path of the program that would be executed for a particular command\n\n## environment variables\n\n- variables that are automatically set when you start the shell\n- example: <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">path</code>\n\n## interesting stuff from [missing semester](https://missing.csail.mit.edu/2020/course-shell/)\n\n- almost all files in <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">/bin</code> have a <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">x</code> permission set for the last group\n    - so that anyone can execute those programs\n- sudo - do as su(superuser)\n- using the sys directory we can interact and do cool stuff with hardware\n  - [controling the backlit programtically](https://missing.csail.mit.edu/2020/course-shell/#:~:text=a%20versatile%20and%20powerful%20tool)\n  \n## flags vs options\n\n- if we pass some value with a flag - option\n- here <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">-p</code> (don't follow symlinks) is a flag and <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">iname</code> is an option\n\n```bash\nfind -P -iname '*some_string*'\n```\n\n## streams\n\n- in shell, every program have input and output streams, by default set to terminal\n- this can be configured using <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">< file</code> and <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">> file</code>\n- this  command reads from hello1 and writes to hello2\n\n```bash\ncat < hello1.txt > hello2.txt\n```\n\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">>> file</code> will append instead of overwriting to a file\n- can be also used for streaming media files when combined with [chormecast](https://vitux.com/how-to-cast-video-from-ubuntu-to-chromecast/)\n\n## strings\n\n- defined with <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">'</code> are string literals\n- defined with <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">\"</code> will expand and substitute variables\n- [bash quoting](https://www.gnu.org/software/bash/manual/html_node/Quoting.html)\n\n## special variable in bash scripting\n\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">$0</code> - Name of the script\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">$1</code>to <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">$9</code> - Arguments to the script. <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">$1</code> is the first argument and so on.\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">$@</code> - All the arguments\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">$#</code> - Number of arguments\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">$?</code> - Return code of the previous command\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">$$</code> - Process identification number (PID) for the current script\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">!!</code> - Entire last command, including arguments\n    - can be useful if we realize that the command needs root privileges after executing it for the first time\n    - just type <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">sudo !!</code> followed by a <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">tab</code>\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">$_</code> - Last argument from the last command\n\n## exit codes and short-circuiting\n\n```bash\nfalse || echo \"Oops, fail\"\n# Oops, fail\n\ntrue || echo \"Will not be printed\"\n#\n\ntrue && echo \"Things went well\"\n# Things went well\n\nfalse && echo \"Will not be printed\"\n#\n\ntrue ; echo \"This will always run\"\n# This will always run\n\nfalse ; echo \"This will always run\"\n# This will always run\n```\n\n## wildstars and globbing\n\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">?</code> - to match one character only\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">*</code> - to match any amount of characters\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">{}</code> - expand command using a list of arguments\n\n```bash\nconvert image.{png,jpg}\n# Will expand to\nconvert image.png image.jpg\n\n# Globbing techniques can also be combined\n\n# This will move all *.py and *.sh files\nmv *{.py,.sh} folder\n\nmkdir foo bar\n# This creates files foo/a, foo/b, ... foo/h, bar/a, bar/b, ... bar/h\ntouch {foo,bar}/{a..h}\ntouch foo/x bar/y\n\n# Show differences between files in foo and bar\ndiff <(ls foo) <(ls bar)\n# Outputs\n# < x\n# ---\n# > y\n```\n\n## scripting vs functions\n\n- functions should be in the same language\n- scripts can be in any language\n  - env can be specified using the <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">shebang</code>\n  - a good practice is to give env name as an argument, rather than the direct path    \n  - ```bash\n    #! usr/bin/env env_name\n    ```\n<br>\n\n- functions are loaded only for the first time ⇒ they are faster\n- scripts are loaded every time they are executed\n\n<br>\n\n- functions are loaded in the current shell environment\n    - functions can modify environment variables\n- scripts are executed in their separate processes\n\n## tools\n\n- [ ] <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">TLDR</code> pages or <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">bro</code> pages instead of <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">man</code> pages\n- [ ] for finding and performing ops on file\n    - [ ] <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">find</code> command\n    - [ ] <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">fd</code> - better alternative\n    - [ ] <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">locate</code> - uses indexing\n\n\n","n":0.035}}},{"i":9,"$":{"0":{"v":"Intro","n":1},"1":{"v":"\n## Shell vs Terminal\n\n- kernel - server\n- shell - middleware\n- terminal - client\n\n## Directory Structure\n\n![](/assets/images/2023-01-05-18-13-15.png)\n\n### `/bin`\n\n- contains binaries and executable programs\n- most of the command that we use are inside this directory\n\n<blockquote style=\"background-color: #FF313120; padding:4px 3px; border-radius: 5px; border-left: 0.25em solid #FF3131; padding-left: 0.75em\">cd is not a binary, it is built in shell command</blockquote>\n\n- `/sbin` contains kernel mode binaries\n\n### `/boot`\n\n- contains static boot loader files\n- `GRUB` files are here\n  \n### `/var`\n\n- contains log files, packages and database files\n\n### `/usr` - user system resources\n\n- non-essential binaries are here\n\n### `/dev`\n\n- terminal devices, disk drives, USB's are included here\n\n### `/etc`\n\n- contains system configuration files\n- should be text files, cannot be executables\n\n### `/mnt`\n\n- all external drivers are available here\n\n## Shell Variable\n\n- default datatype is string\n- are accessed using `$`\n\n### Some builtins\n\n- `PATH`\n- `?`: returns the status code of last-entered command\n\n## A Script file\n\n<blockquote style=\"background-color: #0096FF20; padding:4px 3px; border-radius: 5px; border-left: 0.25em solid #0096FF; padding-left: 0.75em\">in linux, extension does not matter</blockquote>\n\n- kernel uses the interpreter mentioned in the `shebang`\n\n```bash\n!# usr/bin/env bash\n```\n\n- status code of script is the status code of the last executed command\n\n## Trivia\n\n- bash: bourne again shell\n- pwd: print working directory\n\n## Some useful manuals\n\n- `man`\n- `whatis`: one line description of commands\n- `apropos`: when you don't know the exact spelling\n\n## Access Right Code\n\n![](/assets/images/2023-01-05-18-31-23.png)\n\n- meaning for files\n  - `r`: can read and copy\n  - `w`: can change the file\n  - `x`: can execute the file\n- meaning for directory\n  - `r`: can list files in the directory\n  - `w`: can delete / move files in the directory\n  - `x`: can access files, provide you have access to those individual files\n\n## Foreground and Background processes\n\n[stackexchange](https://unix.stackexchange.com/questions/175741/what-is-background-and-foreground-processes-in-jobs)\n\n- first answer\n","n":0.061}}},{"i":10,"$":{"0":{"v":"Capture","n":1},"1":{"v":"\n- hierarchy for TBD notes\n","n":0.447}}},{"i":11,"$":{"0":{"v":"Dev","n":1}}},{"i":12,"$":{"0":{"v":"Python","n":1}}},{"i":13,"$":{"0":{"v":"Functional Programming","n":0.707}}},{"i":14,"$":{"0":{"v":"Generators","n":1},"1":{"v":"# What is a generator?\n\n1. generator is a function:\n   - that returns an object when you call `next()`\n     - stores the state between multiple `next()` executions\n     - `next()` can be called until we encounter a `StopIteration`\n2. generator is a function:\n   - that behaves like an `iterator`\n\n# Example(use case)\n\n- we write a program that makes a list of first\n\n## The non-efficient way\n\n```python\ndef first_n(n):\n    '''Build and return a list'''\n    num, nums = 0, []\n    while num < n:\n        nums.append(num)\n        num += 1\n    return nums\n   \n   \nsum_of_first_n = sum(first_n(1000000))\n```\n\n- this will create the entire list in memory\n  - **_would be very bad if we something other than integer(ex: file)_**\n\n## Efficient Custom Iterator way\n\n- adding the `__iter__` and `__next__` methods\n- sum will now take values using the `next()`\n\n```python\nclass first_n(object):\n\n\n    def __init__(self, n):\n        self.n = n\n        self.num = 0\n\n\n    def __iter__(self):\n        return self\n\n\n    def __next__(self):\n        if self.num < self.n:\n            cur, self.num = self.num, self.num+1\n            return cur\n        raise StopIteration()\n\n\nsum_of_first_n = sum(first_n(1000000))\n```\n\n- this is memory efficient, but there is still a lot of boilerplate code\n\n## Efficient Generator Way\n\n```python\ndef firstn(n):\n    num = 0\n    while num < n:\n        yield num\n        num += 1\n\nsum_of_first_n = sum(firstn(1000000))\n```\n\n# Understanding Generator\n\n- generator looks like a normal function, except for the `yield` statement\n  - `yield` sends the value back to the caller like return, **_but the function_** is not exited yet\n  - the **state** of the function is stored until the next `next()` function call (implicitly using `for` or explicitly using `next()`)\n  \n## Understanding the `yield` statement\n\n- the `next()` call on a generator object triggers the execution of function until a `yield` statement is encountered\n  - on encountering `yield` the value is returned and that **state** is saved\n  - this includes\n    - any local variables\n    - instruction pointer\n    - internal stack\n    - exception handling\n- the next `next()` continues the execution of function right below the `yield statement`\n  - if there is no next yield in the function, a `StopIteration` error would be thrown\n\n```python\n>>> def multi_yield():\n...     yield_str = \"This will print the first string\"\n...     yield yield_str\n...     yield_str = \"This will print the second string\"\n...     yield yield_str\n...\n>>> multi_obj = multi_yield()\n>>> print(next(multi_obj))\nThis will print the first string\n>>> print(next(multi_obj))\nThis will print the second string\n>>> print(next(multi_obj))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\n```\n\n# Tradeoffs (memory vs speed)\n\n- generators are memory efficient but can be slow\n- [generator vs list comprehension -- profiling](https://realpython.com/introduction-to-python-generators/#profiling-generator-performance)\n\n# Further Reading\n\n- [real python article](https://realpython.com/introduction-to-python-generators/#profiling-generator-performance)\n- [ ] advanced generator methods\n  - [ ] `.send()`\n  - [ ] `.throw()`\n  - [ ] `.close()`\n- [ ] use (data pipeline)\n","n":0.049}}},{"i":15,"$":{"0":{"v":"Asynchrnous Programming","n":0.707},"1":{"v":"\n# Asynchronism\n\n- occurrence of events independent of the main program flow\n  - not limited to multiple threads / processes\n\n## Why Async?\n\n![](/assets/images/2023-01-09-18-49-42.png)\n\n- due to the `GIL`, python will have multiple threads at single time, but it still runs a single **CPU instruction** at single time\n  - there are ways to go around the `GIL` with `multiprocessing` and `c-python`\n\n<blockquote style=\"background-color: #0096FF20; padding:4px 3px; border-radius: 5px; border-left: 0.25em solid #0096FF; padding-left: 0.75em\">Python runs thread concurrently, but not in parallel</blockquote>\n\n<details>\n<summary>concurrent and parallel</summary>\n\n![](/assets/images/2023-01-10-11-38-43.png)\n\n</details>\n\n- in general:\n  - multithreading is good for **io-bound** tasks\n  - multiprocessing is good for **cpu-bound** tasks\n\n### Generator Analogy for async\n\n- [[capture.dev.python.functional-programming.generators]] does work in chunks of multiple `next()` executions\n  - similarity: work is picked-up where we last left the execution in `async` and in `generator`\n  - `async` would give the control to some other `coroutine` when it is being waited\n\n# Example\n\n- [code link](https://github.com/mikeckennedy/async-await-jetbrains-webcast/tree/master/code/producer_consumer)\n\n<details>\n<summary>Performance Improvements</summary>\n\n![](/assets/images/2023-01-10-13-33-13.png)\n\n![](/assets/images/2023-01-10-13-33-27.png)\n\n</details>\n\n## Async function anatomy\n\n![](/assets/images/2023-01-10-13-32-24.png)\n\n# Common methods of asyncio\n\n- `coroutines`\n- `asyncio.run()`\n- `asyncio.wait_for()`\n- `execute`\n- `task` and `create_task`\n- loops\n  - `get_event_loop`\n  - `run_until_complete`\n  - `close`\n\n# TODOs\n\n- remaining part from jetbrains videos\n- comparison with js (futures and promises)\n- ten thousand meters blog\n  \n- [python docs](https://docs.python.org/3/library/asyncio.html)\n  - can start from coroutines and tasks part\n","n":0.073}}},{"i":16,"$":{"0":{"v":"Python Asyncio","n":0.707},"1":{"v":"- python async IO is not threading, nor is it multiprocessing\n- async IO is a single-threaded, single-process design: it uses `cooperative multitasking`\n\n<details>\n<summary>Simultaneous chess playing Example</summary>\n\n- Chess master Judit Polgár hosts a chess exhibition in which she plays multiple amateur players. She has two ways of conducting the exhibition: synchronously and asynchronously.\n- Assumptions:\n  - 24 opponents\n  - Judit makes each chess move in 5 seconds\n  - Opponents each take 55 seconds to make a move\n  - Games average 30 pair-moves (60 moves total)\n- **Synchronous version**: Judit plays one game at a time, never two at the same time, until the game is complete. Each game takes **(55 + 5) * 30 == 1800** seconds, or 30 minutes. The entire exhibition takes **24 * 30 == 720** minutes, or **12 hours**.\n- **Asynchronous version**: Judit moves from table to table, making one move at each table. She leaves the table and lets the opponent make their next move during the wait time. One move on all 24 games takes Judit **24 * 5 == 120** seconds, or 2 minutes. The entire exhibition is now cut down to **120 * 30 == 3600** seconds, or just **1 hour**. [(Source)](https://youtu.be/iG6fr81xHKA?t=4m29s)\n\n</details>\n\n## Rules of asyncio\n\n- syntax `async def` introduces either a **native coroutine** or an **asynchronous generator**\n  - expressions `async with` and `async for` are also valid\n- keyword `await` passes function control back to the event loop.\n\n```python\nasync def g():\n    # Pause here and come back to g() when f() is ready\n    r = await f()\n    return r\n```\n\n<details>\n<summary>explanation</summary>\n\n- If Python encounters an `await f()` expression in the scope of `g()`, this is how `await` tells the event loop, “Suspend execution of `g()` until whatever I’m waiting on—the result of `f()` —is returned. In the meantime, go let something else run.”\n\n</details>\n\n- function that you introduce with `async def` is a **coroutine**\n  - To call a coroutine function, you must `await` it to get its results\n  - It is less common (and only recently legal in Python) to use `yield` in an `async def` block.\n    - This creates an [asynchronous generator](https://www.python.org/dev/peps/pep-0525/), which you iterate over with `async for`.\n  - Anything defined with `async def` may not use `yield from`, which will raise a `SyntaxError`.\n- it is a `SyntaxError` to use `await` outside of an `async def` coroutine.\n\n<details>\n<summary>code examples</summary>\n\n```python\nasync def f(x):\n    y = await z(x)  # OK - `await` and `return` allowed in coroutines\n    return y\n\nasync def g(x):\n    yield x  # OK - this is an async generator\n\nasync def m(x):\n    yield from gen(x)  # No - SyntaxError\n\ndef m(x):\n    y = await z(x)  # Still no - SyntaxError (no `async def` here)\n    return y\n```\n\n</details>\n\n- when you use `await f()`, it’s required that `f()` be an object that is [awaitable](https://docs.python.org/3/reference/datamodel.html#awaitable-objects)\n- an awaitable object is either\n  1. another coroutine or\n  2. an object defining an `.__await__()` dunder method that returns an iterator\n\n## Design Patterns\n\n","n":0.046}}},{"i":17,"$":{"0":{"v":"Archive","n":1},"1":{"v":"- hierarchy for backlogged / incomplete notes\n","n":0.378}}},{"i":18,"$":{"0":{"v":"Acads","n":1}}},{"i":19,"$":{"0":{"v":"Sem 6","n":0.707}}},{"i":20,"$":{"0":{"v":"Machine Learning","n":0.707}}},{"i":21,"$":{"0":{"v":"Decision Trees","n":0.707},"1":{"v":"- non parameterized supervised learning algorithm\n- we divide the predictor space into number of simple regions\n- prediction is made on the basis of some statistic of these regions\n- decision trees are easier to interpret, but its performance is not competitive with other algorithms\n  - bagging, boosting, random forests are used to improve performance\n\n## Regression Trees\n\n- we divide predictor space (data: $X_1, X_2, ... X_p$) into $J$ distinct non-overlapping regions($R_1, R_2, ... R_j$)\n- for each region we make the same prediction: **mean** or **median** of that region\n\n### How to construct the regions\n\n- we try to minimize the objective\n- ![](/assets/images/2023-01-16-15-43-31.png)\n- we use `recursive binary splitting`\n  - i.e. at every point we try to divide a particular space around a cutpoint $s$: $\\{X| X_j < S\\}$ and $\\{X| X_j > S\\}$\n  - there are [ways](https://mlcourse.ai/book/topic03/topic03_decision_trees_kNN.html#how-a-decision-tree-works-with-numerical-features) to define a set of cutpoints\n  - we find a cutpoint such that the sum of objective in the sub-regions is minimzed\n\n#### Overfitting and Tree Pruning\n\n- we first build a very large tree\n- we use cross validation to find subtrees that minimze the test error\n- we introduce $\\alpha$ -- the complexity parameter\n  - for every value of alpha there is a subtree such that the objective is minimum\n  - ![](/assets/images/2023-01-16-16-31-27.png)\n  - $|T|$ indicates the number of terminal nodes\n- when $\\alpha$ is 0, we get the original large tree\n- but as $\\alpha$ increases, branch gets pruned from tree in a nested and predictable fashion\n\n- we then figure out the optimal alpha using cross validation\n\n## Classification Trees\n\n- same approach as regression trees\n  - final prediction is based on the most occurring class in the region\n\n### Error functions for classification trees\n\n- mis classification rate\n- ![](/assets/images/2023-01-16-16-39-45.png)\n  - $p̂mk$ = proportion of training observations in the mth region that are from the kth class\n- gini index\n  - ![](/assets/images/2023-01-16-16-49-06.png)\n- entory\n  - ![](/assets/images/2023-01-16-16-49-18.png)\n- gini index and entropy are both related to variance and show similar results\n","n":0.057}}},{"i":22,"$":{"0":{"v":"Cs","n":1}}},{"i":23,"$":{"0":{"v":"Dl","n":1}}},{"i":24,"$":{"0":{"v":"Pytorch","n":1}}},{"i":25,"$":{"0":{"v":"Quickstart","n":1},"1":{"v":"\n- [ ] data loading\n  - [ ] why and how colate numpy\n- [ ] model\n  - shapes shapes shapes\n- [ ] tain and eval utils\n  - [ ] E - changes in eval functions\n- [ ] train function\n  - [ ] toch.optim -> optax\n    - [ ] init params\n    - [ ] loss function\n    - [ ] backprop / update\n","n":0.128}}},{"i":26,"$":{"0":{"v":"Os","n":1}}},{"i":27,"$":{"0":{"v":"Intro","n":1},"1":{"v":"- to become a good developer, u need to get hours under the curve\n- theories are beautiful, but u wouldn't go far without implementation\n\n- what is operating system\n  - computer program\n    - multiplexes the hardware resources\n      - allows multiple people/program to use memory, disks, networks...\n    - implements useful abstractions\n      - process, threads, address space, files, sockets\n- why study os\n  - no interaction\n  - bad language(intro os ass)\n  - to study the beautiful softwares\n  - science part of computer science\n- structure\n  - concepts\n  - doing\n- tannenbaoum book\n\n- asst0\n-\n","n":0.107}}},{"i":28,"$":{"0":{"v":"Acads","n":1}}},{"i":29,"$":{"0":{"v":"Sem 7","n":0.707}}},{"i":30,"$":{"0":{"v":"Aca","n":1}}},{"i":31,"$":{"0":{"v":"Intro","n":1},"1":{"v":"\n- [Lecture 0](#lecture-0)\n  - [moore fsm](#moore-fsm)\n  - [sequence detector](#sequence-detector)\n- [Lecture 2](#lecture-2)\n  - [arbiter](#arbiter)\n    - [steps](#steps)\n  - [hamming Code (1bit error correcting code)](#hamming-code-1bit-error-correcting-code)\n  - [CRC (cyclic redundancy check)](#crc-cyclic-redundancy-check)\n- [Lecture 3](#lecture-3)\n  - [Types of CRC implementation](#types-of-crc-implementation)\n    - [Combinational](#combinational)\n    - [Iterative](#iterative)\n      - [Rolled Version](#rolled-version)\n      - [Unrolled Version](#unrolled-version)\n    - [Setup Time in Flip-Flops (for iterative versions)](#setup-time-in-flip-flops-for-iterative-versions)\n- [Lecture 4](#lecture-4)\n  - [LFSR (linear feedback shift register)](#lfsr-linear-feedback-shift-register)\n    - [Standard Format](#standard-format)\n    - [Modular Format](#modular-format)\n      - [Differences](#differences)\n    - [LFSR based CRC](#lfsr-based-crc)\n\n# Lecture 0\n\n## moore fsm\n\n![](/assets/images/2022-11-21-11-01-34.png)\n\n- it is a sequential circuit\n- combinational circuit followed by flip-flops\n- *next state is a function of present state*\n\n## sequence detector\n\n![](/assets/images/2022-11-21-11-15-18.png)\n\n- use cases: *sender and receiver (CN)*, to check for some code(error code)\n- steps:\n  - start with reset($S_0$)\n  - keep expanding leaves\n  - discard leaves whose pattern is not required and connect back\n- [ ] ***practice some random sequence detector***\n\n# Lecture 2\n\n## arbiter\n\n- allocate access to shared resources\n- consider a shared memory and 3 processes\n  - input: [r1, r2, r3]\n  - output: [a1, a2, a3]\n  - arbiter gives ack to one of the process at a time\n- types: **priority** based, **history** based\n\n### steps\n\n- write list of states describing the current state of the system\n  - used, waiting, not waiting\n- [ ] [Arbiter Practice](https://drive.google.com/drive/u/1/folders/14yvGxRrt4DxzK01nqcRt2KcST_RHMpMS)\n- [ ] ***write states for priority and history based***\n- [ ] ***write next states for some random states in both cases***\n\n## hamming Code (1bit error correcting code)\n\n- steps\n  - sender\n    1. figure out the number of parity bits required ($2^p \\geq m + p + 1$)\n    2. make the structure for the encoded message\n    3. form groups and find the value of parity bits\n       1. parity bit = `xnor` of all message bits in its group\n  - receiver\n    - write all groups with parity bit\n    - write 1 against group with even number of 1s\n    - ***combined bits(in reverse) gives the position of the error***\n- [ ] [Arbiter Practice](https://drive.google.com/drive/u/1/folders/1OZRbEKXlzcwTNG6a1cZO0YHb-327ovGH)\n- [ ] ***practice 2 messages (with and without error)***\n\n## CRC (cyclic redundancy check)\n\n- divisor(generate) polynomial\n- dividend(message) polynomial\n- ***Encoded message: for a k-bit divisor select (k-1) bit from remainder and append to the end of dividend***\n- additionally, there is an look-up table to find the position of bit\n\n# Lecture 3\n\n- sequential => involvement of clock\n\n## Types of CRC implementation\n\n### Combinational\n\n<details>\n<summary>Diagram</summary>\n\n![](/assets/images/2022-11-21-14-27-22.png)\n\n</details>\n\n- separate hardware for separate stages\n- each continuously driving values from previous ones\n- we will get output in single cycle\n  - but the delay will be very large (delay of 8 `XOR` gates)\n  - might not be possible to connect with other components with large delay\n\n### Iterative\n\n- additional hardware is required\n  - register to store the answer of previous stage\n  - hardware for counter(kmap or adder)\n\n#### Rolled Version\n\n<details>\n<summary>Diagram</summary>\n\n![](/assets/images/2022-11-21-14-27-59.png)\n\n</details>\n\n- time period is small / frequency is high\n- less `XOR` gates are required\n- we have to wait for 8 cycles to get an answer\n\n#### Unrolled Version\n\n<details>\n<summary>Diagram</summary>\n\n![](/assets/images/2022-11-21-14-28-17.png)\n\n</details>  \n\n- pipelined version\n- more `XOR` and flip-flops are required\n- new input can be fed every cycle\n- output is also obtained every cycle\n  - initially -- we will have to weight for 8 cycles\n\n### Setup Time in Flip-Flops (for iterative versions)\n\n<details>\n<summary>Diagram</summary>\n\n![](/assets/images/2022-11-21-14-28-43.png)\n\n</details>\n\n- setup delay is common and is required for every\n  - between rolled and combinational, combinational takes less time\n    - combinational: $T_{clkq} + 8 * T_{xor} + T_{setup}$\n    - iterative: $T_{clkq} + 8 * T_{xor} + 8 * T_{setup}$\n  - ***still rolled is preferred, as frequency of combinational is very large and can cause problem in connecting***\n\n- [ ] ***practice a sum on CRC***\n- [ ] ***revise the names and comparison of 3 formats***\n\n# Lecture 4\n\n## LFSR (linear feedback shift register)\n\n- uses: *random number generator*\n\n- steps:\n  - from the generic circuit, create `polynomial` specific circuit\n  - 0 => short, 1 => keep xor gate\n- example: $1 + x^2 + x^3 + x^4 + x^n$\n\n### Standard Format\n\n- next state equation\n  - shift every bit to right\n  - xor the ones with XOR gate before it\n\n<details>\n<summary>general</summary>\n\n![](/assets/images/2022-11-21-23-12-07.png)\n\n</details>\n\n<details>\n<summary>converted for example</summary>\n\n- ![](/assets/images/2022-11-21-19-36-11.png)\n\n</details>\n\n### Modular Format\n\n- next state equation\n  - shift every bit to right\n  - for LSB: xor MSB with registers having XOR gates before them\n\n<details>\n<summary>general</summary>\n\n- ![](/assets/images/2022-11-21-19-34-34.png)\n\n</details>\n\n<details>\n<summary>converted for example</summary>\n\n![](/assets/images/2022-11-21-23-14-29.png)\n\n</details>\n\n#### Differences\n\n- looking at the structures:\n- in modular format, maximum delay is 1 `XOR` only\n- in standard format, this delay can be larger(more than 1 `XOR`)\n\n<blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">non-zero seed would not work</blockquote>\n\n### LFSR based CRC\n\n<details>\n<summary>structure</summary>\n\n![](/assets/images/2022-11-22-00-05-36.png)\n\n</details>\n\n<details>\n<summary>example</summary>\n\n![](/assets/images/2022-11-22-00-05-04.png)\n\n</details>\n\n- one additional `XOR` gate at the input\n- steps\n  - LFSR is initialized to all zeros\n  - We keep passing a single bit, starting from the MSB of `msg` + `0_padded`\n  - we update the LFSR values based on the equation(created using generate polynomial)\n\n- [ ] ***derive specific circuit from general***\n- [ ] ***derive equations and work out LFSR CRC***\n","n":0.036}}},{"i":32,"$":{"0":{"v":"Adders","n":1},"1":{"v":"\n- [Abbreviations](#abbreviations)\n- [Lecture 5](#lecture-5)\n- [Full Adders vs Half Adders](#full-adders-vs-half-adders)\n- [Full adder with GPK](#full-adder-with-gpk)\n- [Full Adder Implementations](#full-adder-implementations)\n  - [Ripple Carry Adder](#ripple-carry-adder)\n  - [Using Generate and Propagate](#using-generate-and-propagate)\n    - [Expanding equations for carry out (carry look ahead adders)](#expanding-equations-for-carry-out-carry-look-ahead-adders)\n- [What impacts the delay of GATES](#what-impacts-the-delay-of-gates)\n- [Lecture 6](#lecture-6)\n  - [Look-ahead adder](#look-ahead-adder)\n  - [Hierarchial look-ahead adder](#hierarchial-look-ahead-adder)\n  - [Think / Revise about](#think--revise-about)\n- [Lecture 7, 8](#lecture-7-8)\n  - [Carry Skip Adders](#carry-skip-adders)\n  - [Hierarchial CSA](#hierarchial-csa)\n  - [Carry Select Adders](#carry-select-adders)\n    - [Conditional Sum Adder](#conditional-sum-adder)\n- [Lecture 9](#lecture-9)\n  - [Bit Serial Adder](#bit-serial-adder)\n  - [General Structure for Adders using generate and propagate](#general-structure-for-adders-using-generate-and-propagate)\n- [List of Multipliers](#list-of-multipliers)\n- [Lecture 10 (Intro CSA)](#lecture-10-intro-csa)\n- [Lecture 11 (CSA Numerical \\&\\& trick for signed numbers)](#lecture-11-csa-numerical--trick-for-signed-numbers)\n- [Lecture 12 (CSA Signed Number numerical \\& Booth re-coding)](#lecture-12-csa-signed-number-numerical--booth-re-coding)\n- [Lecture 13 (LSM, RSM, booth example(4, 8, CSA signed))](#lecture-13-lsm-rsm-booth-example4-8-csa-signed)\n- [Lecture 14 (Baugh Wooley Multiplier \\& fixed point notation)](#lecture-14-baugh-wooley-multiplier--fixed-point-notation)\n- [Lecture 15](#lecture-15)\n- [Lecture 16](#lecture-16)\n\n# Abbreviations\n\n- RCA (ripple carry adder)\n- CLA (carry look-ahead adder)\n- CSA (carry skip adders)\n- CSA (carry select adders)\n- BSA (bit serial adder)\n- PTA (prefix tree adder)\n- CSA (carry save adder)\n\n# Lecture 5\n\n# Full Adders vs Half Adders\n\n- half adders\n  - `sum` = a `xor` b\n  - `carry` = a * b\n- full adders\n  - `sum` = (a `xor` b) `xor` c\n  - `carry` = ab + bc + ac = MAJ(a, b, c)\n    - MAJ = majority gate\n\n# Full adder with GPK\n\n![](/assets/images/2022-11-22-11-26-40.png)\n\n- g = generate (irrespective of C_in, C_out will be 1)\n- p = propagate (C_out = C_in)\n- k = kill (irrespective of C_in, C_out will be 0)\n\n- equations\n  - g = a `and` b\n  - p = a `xor` b\n  \n# Full Adder Implementations\n\n<details>\n<summary>sum of products implementation (DNW)</summary>\n\n![](/assets/images/2022-11-22-11-27-17.png)\n\n</details>\n\n<details>\n<summary>using half adders (DNW)</summary>\n\n![](/assets/images/2022-11-22-11-27-39.png)\n\n</details>\n\n## Ripple Carry Adder\n\n![](/assets/images/2022-11-22-11-28-03.png)\n\n- acts as subtractor when `m = 1`\n  - xor gates give 1's complement of B\n  - M is also C_0, indirectly adds 1 => we get 2's complement of B\n- overflow\n  - only check this when `a` and `b` are in `signed 2's representation', and both are +ve or -ve\n  - do not check in this case\n- carry flag\n  - when `a` and `b` are `unsigned`\n\n## Using Generate and Propagate\n\n- substituting the values of P and G in equations of full adders\n  - `sum` = P `xor` C\n  - `carry` = G + P * C\n    - carry is one when\n      - current stage is generate or\n      - current is propagate and incoming carry is one\n\n### Expanding equations for carry out (carry look ahead adders)\n\n![](/assets/images/2022-11-22-11-28-37.png)\n\n# What impacts the delay of GATES\n\n- number of inputs\n- fan-outs (how many times a output is used again)\n- strength of gate (ignore)\n- pattern of inputs\n\n<details>\n<summary>glitchy behavior</summary>\n\n![](/assets/images/2022-11-22-11-29-43.png)\n\n</details>\n\n- [ ] ***derive RCA equation -> GPK equation***\n- [ ] ***multiple stage c_out equation***\n\n# Lecture 6\n\n## Look-ahead adder\n\n- we get `p` and `g` from `a` and `b` in one get delay\n- and all the c_out can be obtained from `p` and `g` in two gate delay(`and` & `or`)\n- finding `sum` will need another one-gate delay of `XOR`\n  - **thus carry look-ahead is faster than ripple carry adder**\n  - **cost is additional circuits to figure out c_outs**\n  - **layers are roughly independent of the no. of bits we add**\n\n## Hierarchial look-ahead adder\n\n![](/assets/images/2022-11-23-11-21-50.png)\n\n- we make groups of 4 to add one more layer\n- we do this to avoid using large gates\n\n<details>\n<summary>equations</summary>\n\n![](/assets/images/2022-11-23-11-22-19.png)\n\n![](/assets/images/2022-11-23-11-22-41.png)\n\n</details>\n\n<details>\n<summary>3 level 64 bit adder</summary>\n\n![](/assets/images/2022-11-23-11-23-14.png)\n\n</details>\n\n## Think / Revise about\n\n- [ ] ***is the block generating carry or `g`?***\n- [ ] ***how many instances of smaller blocks would be used?***\n- [ ] ***which carry / `g` is generated by which block?***\n\n# Lecture 7, 8\n\n## Carry Skip Adders\n\n<details>\n<summary>simple CSA circuit</summary>\n\n![](/assets/images/2022-11-23-10-58-23.png)\n\n</details>\n\n- ***general strategy to speed-up is to figure out the carry faster***\n- for any stage if propagate signal is set, we skip carry to next stage\n\n## Hierarchial CSA\n\n<details>\n<summary>circuit diagram</summary>\n\n![](/assets/images/2022-11-23-10-51-25.png)\n\n</details>\n\n- for 32 bit CSA, we have 2 levels of skipping (8bit and 32 bit)\n- for 128 bit CSA, we have 3 levels of skipping (8bit, 32bit and 128 bit)\n- ***if we are using RCA of `x` bit, maximum rippling would of size `2x`***\n\n## Carry Select Adders\n\n- we divide a higher order number into lower order groups\n- normally, we would need c_in from groups on left to execute the RCA for groups on right\n- in carry select adders, we find sum for both cases (cin = [0 | 1]) and select after the groups on left have completed\n- for 8 bit num:\n  - RCA would have delay of `8 FA`, requires `8 FA`\n  - CSA would have delay of `4 FA`, requires `12 FA` + some `mux`\n\n<details>\n<summary>circuit diagram</summary>\n\n![](/assets/images/2022-11-23-11-07-23.png)\n\n</details>\n\n- each k bit adder can be implemented using 3 k/2 bit adders\n- ***main idea: all adders work in parallel, and the above idea can be used recursively***\n\n### Conditional Sum Adder\n\n- when we use 1 bit RCAs, the adder is known as conditional sum adder\n\n- [ ] ***workout one addition through circuit***\n- [ ] ***multiple stage c_out equation***\n- [ ] ***carry skip and conditional sum -- numerical [Examples](https://drive.google.com/drive/u/1/folders/1XyUdKoB9A_qtemFwSmVRS0SI1lVFQP2v)***\n\n# Lecture 9\n\n## Bit Serial Adder\n\n<details>\n<summary>circuit</summary>\n\n![](/assets/images/2022-11-23-11-53-34.png)\n\n</details>\n\n- n bit number, we only have a `FA` and a `FF`\n- we would need n cycle to find the answer\n\n## General Structure for Adders using generate and propagate\n\n![](/assets/images/2022-11-23-11-57-32.png)\n\n- we simplify the diagram in terms of prefix tree\n\n<details>\n<summary>4 bit RCA as prefix tree </summary>\n\n![](/assets/images/2022-11-23-12-01-06.png)\n\n</details>\n\nbrent kung\n\n- 3 types of boxes\n- gray: 3 in -> 1 out\n- black: 4 in -> 2 out\n\n- [ ] ***given a diagram, write out some equations***\n\n# List of Multipliers\n\n- unsgined\n  - left shift and right shift (serial multiplication)\n- singed operands\n  - booth recoding (radix 2, 4, 8)\n  - baugh-wooley method\n- squareing\n  - special case of multiplication\n\n# Lecture 10 (Intro CSA)\n\n- adding more than 2 nums\n  - iteratively adding pairs of 2\n  - thinking about binary multiplication\n    - column counting\n    - 10 to 4 counter\n      - 10 -> 3 + 3 + 3 + 1 using 3 FA\n      - 4, 3 -> 2, 2, 1 using 2 FA\n      - columns represent 2^i\n      - keep using full orders until every column has < 3 elements\n      - 3 dot -> use FA else HA\n      - finally use a ripply carry, GA\n    - wallace and dada tree\n      - at every stage we use 3 rows and FA and will 2 ans\n      - 2 ans -> carry save adder\n    - if the expression is of the form y = 2w + 4x + y + 3z\n      - we just shift w by one column\n      - we shift x by 2 cols\n      - when we shift, size of number increases by one\n- advantage of CSA\n  - from 7 * delay of RCA\n  - to 1 * delay of RCA + some delay of FA\n\n# Lecture 11 (CSA Numerical && trick for signed numbers)\n\n- formula for addition of n, x bit numbers\n  - x + log2(n) --> size of accumulator\n  - accumulator for  \n- numerical\n- understanding diag\n  - input 3 numbers from col (0 -> k - 1)\n    - output\n      - 1 number from col (1 -> k) carry vector\n      - 1 number from col (0 -> k - 1) sum vector\n  - for making diag\n    - do one numerical\n    - then do numerical\n\n- comparison with traditional method (iterative, time multiplexing)\n\n- addition using binary tree of CPAs\n  - delay not very bad\n  - size of k (adders increase)\n\n- signed numbers\n  - check msb\n    - if 1 --> negative\n    - else positive\n  - finding value\n    - method 1\n      - find 2's complement (1's complement + 1)\n      - add - sign\n    - method 2\n      - if msb is one treat is -1 * 2 ^ MSB\n      - add others simply\n\n- combination of columns with +/-  \n\n- difference between squaring and multiplying\n  - multiplying is optimized for squaring\n\n- signed numbers for csa\n  - extending signed numbers\n    - copy the MSB to left side\n  - in columns\n    - flip the msb\n    - add -1 in the msb column\n\n# Lecture 12 (CSA Signed Number numerical & Booth re-coding)\n\n- flip msb add -1 in that column\n- to remove -1 -> shift -1 and add 1 in that column\n\n- size of the -ve wala vector\n- max size of single vector + log2(total number of vector)\n\n- start\n  - based on the column of msb, figure out a new vector of size\n    - calculating size\n      - -3 * (2 ^ power of column) --> bits required to represent this\n    - convert 3 -1s to a new vector\n      - this vector is the signed representation of -3 * ...\n    - add the flipped vectors and the -1 wala vector\n\n```text\n- csa steps signed nos\n  - flip MSBs, combine -1s\n  - based on largest input and #input find size of -wala vector\n  - based on -wala size and sum of -1, find vector (signed rep of sum in -wala size)\n  - use sign extension to make every vector of equal bit\n\n- wherever possible reduce rows by moving ones from one row to empty\n```\n\n- multiplication\n  - done using multi operand addition\n\n- optimizing multiplication -> square\n\n- booth\n  - finding value of binary -> decimal\n    - if we have burst of 1s (011110)\n      - final value\n        - = 2 ^ (4 + 1) - 2 ^ 1\n        - = 32 - 2\n        - = 30\n  - booth re-coding (radix 2)\n    - look at window of 2 nums\n      - 00 -> 0\n      - 10 -> -1\n      - 11 -> 0\n      - 01 -> 1\n\n# Lecture 13 (LSM, RSM, booth example(4, 8, CSA signed))\n\n- LSM (a, b)\n  - find bits in ans\n  - 1. start with all 0\n  - 2. ls\n  - 3. add msb\n  - repeat till lsb\n  - advantage\n    - process become iterative\n  - cost\n    - shifter, ffs, adder, and gates, shifter(to find a single bit)\n- RSM (a, b)\n  - find bits in ans + 1\n  - start with all 0\n  - rs\n  - add lsb on right side\n  - repeat and append carry in beginning\n\n- radix 4 booth (signed a, b)\n  - window has overlap of 1 bit with previous window\n  - encode second operand(b)\n    - find possible values of a needed (3, 2, 1, 0, -1, -2, -3)\n  - find bits in ans\n    - start with all 0\n    - 4 booth => shift twice\n    - add a*msb(b_coded)\n  - repeat\n  - ***extension is alway sign extension***\n- radix 8 (signed a, b)\n  - window has overlap of 1 bit\n  - same step as above\n\n- radix with csa signed trick\n- advantage of signed trick over sign extension\n  - reduce size of adder\n\n# Lecture 14 (Baugh Wooley Multiplier & fixed point notation)\n\n- msb negative contributor, others positive\n- take 2's complement of negative contributors and make vectors\n- fill the vectors in empty rows\n- combine 2's wherever possible\n\n- fxp\n  - (bd, ad) -- d = decimal\n  - finding the resolution, largest and smallest numbers\n    - singed and unsigned\n    - where to round off\n\n# Lecture 15\n\n- Divide and Conquer Multiplier (unsigned)\n  - triangle lines wala structure\n  - do simple group multiplication\n    - combine rows wherever possible\n  - steps #1\n    - find multiplication of groups\n    - add using carry save adder\n      - place items carefully\n      - add lsb of both number\n      - msb = lsb + bits req\n  - steps #2\n    - write everything directly\n- clk based carry-save adder\n  - normally\n    - 100 clock cycles, T_cycle is function of no. of bits\n  - with CSA\n    - 100 cycles, size does not impact only 1 FA delay\n- Newton-Raphson Method (approximation)\n  - x_(i+1) = x_i - f(x_i) / f'(x_i)\n  - to find 1/d\n    - f(x) = D - 1/x\n      - x_i+1 = x_i (2 - d.xi)\n      - division in converted to multilier(2) and sub(1)\n  - to find 1/root(some number)\n    - figuring out the fxp req\n\n# Lecture 16\n\n- find root using some other equation\n- what is a good function\n\n- Cordic (circular)\n  - rotating a vector by `a`\n    - x = xcosa - ysina\n    - y = xsina + ycosa\n","n":0.023}}},{"i":33,"$":{"0":{"v":"Sem 6","n":0.707}}},{"i":34,"$":{"0":{"v":"Machine Learning","n":0.707}}},{"i":35,"$":{"0":{"v":"9 Regularization","n":0.707},"1":{"v":"\n# Solutions to overfitting - Regularisation\n\n- we introduce an additional parameter $\\lambda$, which is the *regularisation* parameter or a penalty term\n- ![](/assets/images/2022-02-09-11-33-47.png)\n  - $\\lambda \\geq 0$ is regularization parameter\n    - assigns weight to the *penalty* or *bias*\n  - $c(\\theta)$ is some form of complexity penalty\n    - a common penalty is $C(\\theta) = -\\log p(\\theta)$, where $p(\\theta)$ is the prior probability for $\\theta$\n    - **_we assume that we have some prior knowledge about the problem_**\n- if $l$ is the negative log loss, the reguralised objective becomes\n- ![](/assets/images/2022-02-09-11-43-41.png)\n- setting $\\lambda=1$ and rescaling $p(\\theta)$, we minimize the following\n- ![](/assets/images/2022-02-09-11-45-16.png)\n\n<details>\n<summary>minimizing this objective is equivalent to maximizing the posterior log</summary>\n\n- in *MLE*, we do not consider any prior knowledge and we tweak $\\theta$ to optimize - $P(D | \\theta)$\n- the prior knowledge about the parameter can be expresses as - $P(\\theta)$\n- to incorporate this prior knowledge, we find the posterior porbability $P(\\theta | D)$\n  - using the bayes theorem, $P(\\theta | D) = \\frac{P(D | \\theta)P(\\theta)}{P(D)}$\n  - applying the log, we can get the equation described in the next step\n\n</details>\n\n- ![](/assets/images/2022-02-09-11-51-24.png)\n- this is known as <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">MAP</code> (*maximum a posterior estimation*)\n\n## Bernoulli Example with Regularisation\n\n- to avoid overfitting, we introduce *prior* $p(\\theta)$ as a <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">beta distribution</code>\n\n<details>\n<summary>beta distribution</summary>\n\n- ![](/assets/images/2022-02-09-12-03-48.png)\n- based on the values of a and b, we get different plots for the beta distribution\n- for a, b > 1, we get a dsitribution similar to the purple one, which discourages extreme values of theta like 0 or 1\n\n</details>\n\n- the log likelihoo plus the log prior becomes\n- ![](/assets/images/2022-02-09-12-09-02.png)\n- to find the maximal value, we differentiat and set it 0, to get the value of $\\theta_{map}$\n- ![](/assets/images/2022-02-09-12-11-12.png)\n- setting a = b = 2, which favous the value of $\\theta = 0.5$\n- ![](/assets/images/2022-02-09-12-11-57.png)\n- this is known as *smoothing of frequentist approach*","n":0.057}}},{"i":36,"$":{"0":{"v":"8-MLE-2","n":1},"1":{"v":"\n# Model Fitting or Training\n\n- estimating $\\theta$ from a given data\n  - we estimate it by minimizing the loss function $L(\\theta)$\n\n## Maximum Likelihood Estimation\n\n- we pick the parameters that assign highest probability to the training data\n- we define MLE as\n- ![](/assets/images/2022-02-07-11-43-45.png)\n- we assume that the training examples are independently sampled from the same distribution, ie <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">iid</code>(indpendent and identically distributed)\n- the conditional likelihood then becomes\n- ![](/assets/images/2022-02-07-11-50-56.png)\n  - **_as the examples are independent, the overall probability can be expressed as the product of individual probabilities_**\n- We use a log likelihood:\n- ![](/assets/images/2022-02-07-11-54-31.png)\n  - it is mathematically convenient to use log function as the likelihood can be expessed as summation\n  - $log$ is a monotonically increasing function and provides one to one mapping\n- The *MLE* is then given by:\n- ![](/assets/images/2022-02-07-11-56-51.png)\n\n<br>\n\n- most of the cost functions are designed to *minimize* const function, we can redfine objective function to be a *negative log likelihood* or <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">NLL</code>\n- ![](/assets/images/2022-02-07-11-58-41.png)\n- minimizing this gives us the $\\hat{\\theta_{mle}}$\n\n#### Optimal theta for unsupervised(unconditional) models\n\n- as we only have outputs and no inputs in this case, MLE becomes\n- ![](/assets/images/2022-02-08-08-02-15.png)\n- alternatively, we can maximize the *join likelihood* of inputs and outputs. The MLE in this case becomes\n- ![](/assets/images/2022-02-08-08-07-39.png)\n\n### Example: MLE for Bernoulli distribution\n\n- Y is $RV$ representing a coin toss\n  - Y = 1 corresponds to heads\n  - Y = 0 cooresponds to tails\n- Let $\\theta = p(Y=1)$ be the probability of heads\n- MLE for such a *bernoulli dist.* can be given by:\n- ![](/assets/images/2022-02-08-08-19-06.png)\n- finding the derviative and setting it to 0, we get\n- $\\hat{\\theta_{mle}} = \\frac{N_h}{N_h + N_t}$, which is similar to intuitive result\n\n<br>\n\n- __Here we have not incoporated *population risk* and thus there is a high change of overfitting the training set__\n  - The model in this case has enough parameters to perfectly fit the observed training data","n":0.057}}},{"i":37,"$":{"0":{"v":"7 Multivariance Models","n":0.577},"1":{"v":"# Multivariate Models\n\n- measure of dependence of one $RV$ on others\n  \n## Covariance\n\n- measures the degree of linearity(dependency) between two *random variables*\n- ![](/assets/images/2022-02-02-11-40-37.png)\n- for higher dimension, we define a *covariance matrix* $\\Sigma$\n- ![](/assets/images/2022-02-02-11-44-24.png)\n  - covariance matrix is **_positive definite symmetric matrix_**  \n    - easy to perform eigenvalue decomposition and the eigenvalues are real\n  - range is $[-\\infty, \\infty]$\n\n### Imortant Properties\n\n- ![](/assets/images/2022-02-07-18-39-14.png)\n- ![](/assets/images/2022-02-07-18-39-35.png)\n- ![](/assets/images/2022-02-07-18-40-00.png)\n\n## Correlation(degree of linearity)\n\n- normalizing the covariance, we get *(Pearson) __correlation coefficient__*\n- ![](/assets/images/2022-02-07-18-41-50.png)\n  - range is $[-1, 1]$\n  \n<details>\n<summary>correlation matrix</summary>\n\n![](/assets/images/2022-02-07-18-44-05.png)\n\n</details>\n\n- ![](/assets/images/2022-02-08-07-21-24.png) \n  - $Corr[X, Y] = 1 iff Y = aX + b, a > 0$\n\n#### Relation between autocovariance and autocorrelation\n\n- ![](/assets/images/2022-02-08-07-24-18.png)\n- K is autocorvaiance and R is autocorrelation\n\n<blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\"><b>Uncorrelated does not imply independence</b><br>Corrleation only measure linear dependency. So the data can be completely dependent but may have 0 correlation</blockquote>\n\n<blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">Correlation does not imply causation</blockquote>\n\n### Simpson's Paradox\n\n![](/assets/images/2022-02-08-07-31-20.png)\n\n- trends shown by small groups of data can disappear or reverse when the groups are combined\n  - in small groups, y increases with x, but overall y decreases with x\n\n# TODO\n\n- [ ] mutlivariate gaaussian distribuiton\n- [ ] scripts","n":0.071}}},{"i":38,"$":{"0":{"v":"6 Curse of Dimensionality","n":0.5},"1":{"v":"\n# Curse of Dimensionality\n\n- as the dimension increases, the volume of space grows exponentially\n- *thus we need to look farther to find the nearest neighbour*\n\n## Unit cube example \n\n- we create a space with a constant $n$ number of points, degree $d$ and length 1\n  - $\\implies point\\hspace{1mm}density = \\frac{n}{1^d} = n$\n- we consider another cube with side $s$ \n  - thus the number of points inside this cube is $n*s^d$\n- if we want to find k nearest neighbors, then the length of the cube should be:\n  - $n*s^d = k$\n  - $\\boxed{s = (\\frac{k}{n})^{(\\frac{1}{d})}}$\n- if plot take $k=1$ and find the values of $d$ vs $s$\n  - ![](/assets/images/2022-02-02-01-29-18.png) \n  - i.e. for finding nearest neighbor we have to look at almost the edge of our space\n  - **_in higher dimension, the notion of neighbour is violated_**\n\n<details>\n<summary>another view point</summary>\n\n- ![](/assets/images/2022-02-02-01-43-31.png)\n- [ ] [code](https://github.com/probml/pyprobml/blob/master/scripts/curse_dimensionality_plot.py) \n\n</details>\n\n## Implications\n\n- as $d$ increases, volume of the space increases and data becomes *sparser*\n- to get reliable results, data needs to get exponentially\n- *in high dimension space, objects are sparse and dissimilar*\n- provides motivation for dimensionality reduction techniques like PCA and many more","n":0.073}}},{"i":39,"$":{"0":{"v":"5-KNN","n":1},"1":{"v":"# Examplar based methods\n\n- in *parametric* models, after estimating the parameters from the training data, we no longer need that data\n\n<br>\n\n- in *non-parametric* models, keep the training data\n  - thus the effective number of parameters can grow with size of data $\\lvert\\lvert D \\rvert\\rvert$\n- such models can be defined in terms of dissimilarity or distance function between a point in the testing set $x$ and that in the training set $x_n$ give by $d(x, x_n)$\n- such approach is also called <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">instance-based</code> or <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">memory-based</code> learning\n\n## KNN classification\n\n- **_Assumption: similar datapoints have similar labels_**\n- for a new input $x$, we look at the $K$ nearest neighbors and determine a probability distribution for the label\n- ![](/assets/images/2022-02-02-02-02-47.png)\n- there are two main parameters for *KNN* - number of neigbors $K$ and the distance function $D()$.  \n- **_A small value of k leads to overfitting and a large value of k leads to uncerfitting_**\n\n### choosing the optimal k\n\n![](/assets/images/2022-02-02-02-06-50.png)\n- [ ] [code](https://github.com/probml/pyprobml/blob/master/scripts/knn_classify_demo.py)\n","n":0.077}}},{"i":40,"$":{"0":{"v":"4 Bias Variance Tradeoff","n":0.5},"1":{"v":"\n# Bias Variance Tradeoff\n\n- ![](/assets/images/2022-01-23-13-19-15.png)\n\n## Bias\n\n- average of the difference between the true values and the predicted values\n- it means:\n  - very little attention to training data\n  - oversimplification of model\n- *high bias corresponds to underfitting*\n\n## Variance\n\n- models with high variance pays a lot of attention to training data and does not *generalize*\n- *high variance corresponds to overfitting*\n\n# No Free Lunch Theorem\n\n- <blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">There is no single model that workd optimally for all kinds of problems</blockquote>\n- i.e. if we find the average performance of a model over all possible problems, then there is no single model that outperforms every other models","n":0.094}}},{"i":41,"$":{"0":{"v":"3 Verfitting and Generalisation","n":0.5},"1":{"v":"## Overfitting and generalisation\n\n- empirical risk can be rewrittena as:\n- ![empirical risk](/assets/images/2022-01-17-11-44-05.png)\n- $|D_{train}|$ is the size of training data $D_{train}$\n\n<br>\n\n- model that perfectly fits the training\ndata, but which is too complex - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">overfitting</code>\n\n### Population Risk\n\n- We assume that we have **_access_** to some distribution $p^*(x,y)$ but it is **_unknown_**\n  - i.e. we can **_predict_** but not **_infere_**\n- We define *theoretical expected loss* or <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">population risk</code> as:\n- ![population risk](/assets/images/2022-01-17-11-57-17.png)\n- difference between between population risk and the empirical lisk is defined as the <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">generalisation gap</code>\n  - if a model has high *generalisation gap*, it is a sign of overfitting\n  \n<br>\n\n- As we are only interested in the outcomes to find the *population risk*, we can divide the given data into **_train set_** and **_test set_**\n- *Population Risk* can be then defined as\n- ![PR on test set](/assets/images/2022-01-17-12-02-33.png)\n\n### Choosing the Right Model\n\n- ![Graphs](/assets/images/2022-01-17-12-11-11.png)\n- [ ] [code](https://github.com/probml/pyprobml/blob/master/scripts/linreg_poly_vs_degree.py)\n- from the fourth graph, we can see that as *degree of freedom* increases, the error on training set decreases\n  - but the error on testing set follows a *U-shaped curve*\n  - **Thus we should choose the data that minimizes the loss on test set**\n- <blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">In practice we divide the data into 3 parts<ul><li>validation data - for model selection</li><li>training data - for model training</li><li>testing data- for estimating the performance</li></ul> </blockquote>\n","n":0.064}}},{"i":42,"$":{"0":{"v":"2-MLE-and-regression","n":1},"1":{"v":"## Capturing Uncertainity\n\n- in most of the cases, we can't predict perfectly due to:\n  - *intrinsic(irreducible) uncertainity*: due to error in data collection...\n  - *model uncertainity*: due to absence of infinite data / processing power\n- We capture this uncertainity using conditional probability\n- ![capturing uncertainity](/assets/images/2022-01-16-15-52-52.png)\n- here $f: X \\rightarrow [0, 1]^c$ maps the input to one of the $c$ labels\n  - $f$ follows the axioms of probability:\n    - $0 \\leq f_c \\leq 1$\n    - $\\textstyle\\sum_{c=1}^n f_c = 1$\n- [ ] restriction and <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">softmax function</code>\n- [ ] <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">affine functions</code> and notations\n\n## Maximum Likelihood Estimation\n\n- A common loss function for probabilistic models is *negative log probability*\n- ![NLP](/assets/images/2022-01-16-16-05-35.png)\n- [ ] Intution and reason\n\n### Negative Log Likelihood\n\n- Averaging the of training set gives us <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">NLL</code>\n- ![NLL](/assets/images/2022-01-16-16-07-48.png)\n\n<br>\n\n- If we minimize the *NLL*, we can compute maximum likelihood estimate of <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">MLE</code>\n- ![MLE](/assets/images/2022-01-16-16-08-57.png)\n\n# Regression\n\n- *response* $Y$ is an continuous, real quantity\n\n## Loss Function\n- here the loss function is quadratic - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">l<sub>2</sub> loss</code>\n- ![quadratic loss](/assets/images/2022-01-16-16-11-07.png)\n- quadratic loss penalizes large *residuals* $y-\\hat{y}$ more than small ones \n  - if the data has outliers, quadratic penalty can be very severe\n  - in such cases we can use <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">l <sub>1</sub> loss</code>\n\n## Empirical Risk\n\n- Averraging the quadraticl log function, we get the mean squared error <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">MSE</code>\n- ![MSE](/assets/images/2022-01-16-17-59-02.png)\n\n### Negative Log Likelihood\n\n- Fixing the variance for simplicity and using the $l_2$ loss function, we get *NLL* as:\n- ![NLL](/assets/images/2022-01-16-18-08-38.png)\n- this is proportional to MSE\n  - **hence we can calculate maximum likelihood estimate by minmizing the MSE**\n## Capturing Uncertainity\n\n- In linear regression, output is assumed to *Gaussian* or *Normal*\n- ![Gaussian](/assets/images/2022-01-16-18-02-12.png)\n- Here the mean can be defined using the inputs, $\\mu = f(x_n;\\theta)$, and therefor the probability distribution is given by:\n- ![dist](/assets/images/2022-01-16-18-05-09.png)\n  \n## Linear Regression\n\n- <blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">When we fit the data using <b>linear function of the parameters</b>, it is known as linear regression </blockquote>\n- ![](/assets/images/2022-01-23-12-42-44.png)\n  - [ ] [code](https://github.com/probml/pyprobml/blob/master/scripts/linreg_residuals_plot.py)\n- we can fit a 1d data using a *simple linear regression* model of the form\n- ![](/assets/images/2022-01-23-12-41-37.png)\n- here $w$ is *slope* and $b$ is *offset*, and $\\theta=(w,b)$ are the parameters of the model\n- we minimize the squared error to get the least squares solution\n- ![](/assets/images/2022-01-23-12-39-53.png)\n- If we have multiple *predictors*, we can have a **_multiple lineare regression_**\n- ![](/assets/images/2022-01-23-12-41-00.png)\n\n## Polynomial Regression\n\n- The fitting can be improved by using a **_polynomial regression_** model of the form $f(x;w) = w^T\\phi(x)$\n- $\\phi(x)$ is a featue vector derived from the input \n- ![](/assets/images/2022-01-23-12-47-00.png)\n- ![](/assets/images/2022-01-23-12-55-41.png)\n  - [ ] [code](https://github.com/probml/pyprobml/blob/master/scripts/linreg_2d_surface_demo.py)\n- <blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">It is important that the prediction function is a linear function of the parameter because a linear model induces a loss function MSE(θ) that has <b>unique global optimum</b></blockquote>\n","n":0.045}}},{"i":43,"$":{"0":{"v":"11 Avoiding Overfitting","n":0.577},"1":{"v":"\n## Cross Validation\n\n- when the data is very small, dividing into training, testing and validation set is not a good idea\n  - very small sets can have a high bias\n- we split the training data into **_K folds_**\n  - in a round robin fashion, we train on all except one fold\n  - ![](/assets/images/2022-02-16-11-56-07.png)\n    - $R$ is the *regularized cross-validated risk*\n    - $D_k$ is the data in k'th fold -> train for k'th fold\n    - $D_{-k}$ is the data in k'th fold -> validate for k'th fold\n- this is known as **_K fold cross validation_** or **_leave-one-out cross validation_**\n\n<details>\n<summary>schematic for 5-fold cross validation</summary>\n\n![](/assets/images/2022-02-16-12-04-58.png)\n\n</details>\n\n- if the original data set has imbalance\n  - use weighted strategy, by assigning high weightage to part will smaller size\n- if cross validation causes imbalance\n  - use [stratified cross validation](https://towardsdatascience.com/what-is-stratified-cross-validation-in-machine-learning-8844f3e7ae8e)\n\n## Early Stopping - simple form of regularization\n\n- many of the optimization are *iterative*, so they take many steps to move away from the initial parameter estimates\n- if we detect signs of overfitting by monitoring the performance of validation set, we stop the optimization problem\n- ![](/assets/images/2022-02-16-12-12-55.png)\n- [ ] [code](https://github.com/probml/pyprobml/blob/master/scripts/imdb_mlp_bow_tf.py)\n  \n## Using More Data\n\n- as the data increases, overfitting decreases(assuming the new data adds to the diversity and is not redundant)\n- ![](/assets/images/2022-02-16-12-26-24.png)\n  - black line is true error (noise with variance 4), we cannot go below it -> <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">noise floor</code>\n  - for a small degree, the error always remains high -> *undefitting*\n  - test error decreases for other models, but it reduces more rapidly for simpler models\n  - *generalization gap* is initially larger for complex model, but decreases as size grows\n  ","n":0.061}}},{"i":44,"$":{"0":{"v":"10 Bayesian Statistics","n":0.577},"1":{"v":"## Approaches for Machine Learning\n\n- *frequentist* ($P(D;\\theta)$)\n  - find such a $\\theta$ that maximizes that given data\n  - this approach does not consider $\\theta$ as a random variable\n    - i.e. the value of parameters does not depend on any prior event\n- *bayesian statistics* ($P(D|\\theta)$)\n  - in this approach the value of params depends on prior knowledge\n    - $P(\\theta)$ can be encoded as a distribution\n      - i.e. prior knowledge can be incoporated in the distribution\n\n## Weight Decay\n\n- using regression with very high degree of polynomial results in overfitting\n  - we tried to reduce the degree of polynomial to generalize the model\n- another solutions is to penalize the magnitude of weights(*regression coefficients*)\n\n\n<br>\n\n- we use a *zero-mean Gaussian* prior to penalize the weights, and the resulting MAP is given by\n- ![](/assets/images/2022-02-16-00-09-17.png)\n  - here we use $w$ instead of $\\theta$, since we only penalize the weight vector and not bias terms or noise variances\n- this is called <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">l<sub>2</sub>regularization</code> or **_weight decay_**\n- in case of linear regression, this kind of penalization scheme is called <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">ridge regression</code>\n\n## Finding the optimal regularization parameter\n\n- picking a small $\\lambda \\implies$ focussing on minimising empirical risk $\\implies$ *overfitting*\n- picking a large $\\lambda \\implies$ focussing on prior $\\implies$ *underfitting*\n- we use validation set to find the optimal $\\lambda$\n\n![](/assets/images/2022-02-15-23-59-38.png)\n\n- [ ] [code](https://github.com/probml/pyprobml/blob/master/scripts/linreg_poly_ridge.py)\n\n\n# TODO\n\n- [ ] bias variance dilemma\n- [x] finding validation method for small data\n  - cross validation","n":0.065}}},{"i":45,"$":{"0":{"v":"1-Introduction","n":1},"1":{"v":"# Pillars of the course\n\n- classifcation\n- regression\n- dimensionality reduction\n- clustering\n\n# What is machine learning\n\nJohn Mitchell' definition\n<blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">A computer program is said to learn from an experience <b>E</b>, with respect to some task <b>T</b> and using some performance measure <b>P</b>, if its pefromance improves with experience\n</blockquote>\n\n# Probabilistic approach for ML\n\n- treat every unknown quantity as a <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">RV</code>\n- use different proability distributions to model that uncertainity\n- we use probabilistic approach because:\n  - it is optimal decision making strategy under some uncertainity\n  - other areas of science and engineerig also use the same approach\n\n# Supervised Machine Learning\n\n- data with outputs corresponding to inputs is provided\n- we try to find the mapping between theese inputs and outputs\n\n## Classification Problem\n\n- output space is a set of c *distinct* and *mutually exclusive* <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">labels</code>\n- output is a *categorical variable*\n- the problem is also called <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">pattern recognition</code>\n\n### IRIS flowers dataset\n\n- labels:\n  1. setosa\n  2. versicolor\n  3. virginica\n- featues:\n  1. petal length\n  2. petal width\n  3. sepal width\n  4. sepal length\n\n- In cases similar to this, when the size is fixed, the data is stored using <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">design matrix</code>\n- Such a dataset is <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">tabular data</code>\n- when the inputs are of variable size we may need to store the data in some other format\n\n![Pairwise Scatter Plot for Iris Dataset](/assets/images/2022-01-14-23-01-57.png)\n- pairwise scatter plot for iris dataset\n- the diagonal shows the probability distribution for that particular *predictor*\n- from this we can get intution for decision rules\n  - ex: if petal length exceeds some limit, it won't be of a particular type\n- [ ] [code for the scatter plot](https://github.com/vinaykakkad/pyprobml/blob/master/scripts/iris_plot.py)\n\n### EDA note\n\n- for such datasets with very few predictors, it is common to plot such pair plots to get the intution\n- for datasets with high number of predictors, <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">dimensionality reduction is performed</code>\n  \n![Decision Tree](/assets/images/2022-01-14-23-11-37.png)\n- decision tree for the iris dataset\n- [ ] [code_1 for decision tree](https://github.com/vinaykakkad/pyprobml/blob/master/scripts/iris_dtree.py)\n- [ ] [code_2 for decision tree](https://github.com/vinaykakkad/pyprobml/blob/master/scripts/iris_dtree2.py)\n\n# Empricial(experimental) Risk Minimisation\n\n## Missclassification Rate\n\n- in the above example, error in the model can be expressed as missclassifcation rate which is given by:\n- ![Misclassification Rate](/assets/images/2022-01-16-14-05-56.png)\n  - $\\theta(parameters)$ are the decision rules that we make \n  - in case of linear regression, $\\theta$ are the weights that we assign to the predictors\n- Here $I(e)$ is the binary indicator function, which return when the output return by our model does not match the actual output\n- ![Indicator function](/assets/images/2022-01-16-14-07-57.png)\n\n<br>\n\n- here we assume that each incorrect prediction has the same loss\n- in general, we can define a <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">loss function</code>, which is a function of the predicted output and actual output on the training set\n- the *empirical risk* is then defined as:\n- ![Empirical Risk](/assets/images/2022-01-16-14-22-51.png)\n\n## Model Fitting or Training\n\n- one way is to find $\\theta$ that minimize the empirical risk (*empirical risk minimization*)\n- ![ERM](/assets/images/2022-01-16-14-31-19.png)\n- true goal: *minimize the loss on future data* - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">generalization</code>\n\n# TODOs\n\n- [ ] ISLR\n\n- [ ] Random Variables\n- [ ] Tranformation of RVs\n- [ ] Diff probability distribution\n","n":0.043}}},{"i":46,"$":{"0":{"v":"Fundamentals of Environmental Studies","n":0.5}}},{"i":47,"$":{"0":{"v":"Lec 4","n":0.707},"1":{"v":"\n## minerals\n\n- crystalline(very organized structure)\n- naturally occuring organic substance in the earth's surface\n- basic material for formation of *rock*\n\n<br>\n\n- formed by *volcanic erruptions*\n- crystallization of magma or lava\n- crystallization of materials dissolved in water\n\n## rocks\n\n- naturally formed, consnolidate material, composed of one or more mineral\n  \n### types\n\n- igneous\n  - formed immediatly when magma or lava solidify\n- sedimentary\n  - rock loosened by weathering, transported to some basin or depression where sediment is trapped\n  - sediment become compacted and cemented, forming sedimentary\n  - deposition of once-living organinsms\n- metamorphic\n  - existing rock which get changed due to temperature and pressure\n\n## soil\n\n- comprises of rocks, dead beings(organic matter), water, air int the form of air pockets\n\n### types\n\n- indian soil classificatin\n- ![](/assets/images/2022-01-25-16-55-54.png)\n- classification is generally done on the basis of water percentage, type of content\n\n# Soil Pollution\n\n![](/assets/images/2022-01-25-17-11-02.png)\n- non-targeted: pesticides are intended to kill pests, but they also affects humans and other beings\n\n## Types of pesticides / pollutants\n\n![](/assets/images/2022-01-25-17-13-28.png)\n","n":0.081}}},{"i":48,"$":{"0":{"v":"Lec 2","n":0.707},"1":{"v":"- [Ecosystem](#ecosystem)\n  - [terrestial ecosystems](#terrestial-ecosystems)\n    - [forest ecosystem](#forest-ecosystem)\n      - [tropical evergreen forest](#tropical-evergreen-forest)\n      - [tropical deciduous forest](#tropical-deciduous-forest)\n      - [temperate evergreen forest](#temperate-evergreen-forest)\n      - [temperate deciduous forest](#temperate-deciduous-forest)\n      - [taiga (boreal) forest](#taiga-boreal-forest)\n    - [desert ecosystem](#desert-ecosystem)\n    - [grassland ecosystem](#grassland-ecosystem)\n    - [montane or mountain ecosystem](#montane-or-mountain-ecosystem)\n    - [tundra or alpine ecosystem](#tundra-or-alpine-ecosystem)\n    - [ice ecosystem](#ice-ecosystem)\n- [Biological Terms](#biological-terms)\n  - [importance of mangronves](#importance-of-mangronves)\n\n## Origins of Life\n\n- theory of Urey and Miller\n- in the early days, temperature of earth was very high\n- weather was uncertain, and due to the temp and lightning, something organic came into existence\n\n<br>\n\n- Amino acid are building blocks of protein and protein creates different beings\n\n# Ecosystem\n\n![Ecosystem Map](/assets/images/2022-01-13-17-13-13.png)\n\n- <blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">dynamic complex of biotic components(living beings) and abiotic components(air, water, soil) interacting as a functional unit</blockquote>\n- mainly divided into terrestial and aquatic\n\n\n## terrestial ecosystems\n\n- further divided based on the temperature \n  - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">Tropic Zones</code>\n  - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">Temperate Zones</code>\n      \n\n### forest ecosystem\n\n- high density of flora and fauna\n  \n#### tropical evergreen forest \n\n- mean rainfall of 80 for every 400 inch\n- high diversity of animals (especially insects)\n  \n#### tropical deciduous forest\n\n- equall dry and rainy (30 to 60 inch)\n- shed leaves when dry\n   \n#### temperate evergreen forest\n\n- lower temperature than tropical, high rainfall\n\n#### temperate deciduous forest\n\n#### taiga (boreal) forest\n\n- near arctic\n- evergreen conifers\n- houses are made of wood for conifers\n  - wood - for insulating cold\n  - conifers - for surviving snow\n\n### desert ecosystem\n\n- annual rainfall less than 25 inch\n\n### grassland ecosystem\n\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">savanna</code>: tropical grassland\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">prairies</code>: temperate grassland\n\n### montane or mountain ecosystem\n\n### tundra or alpine ecosystem\n\n- polar regions\n- no trees\n- only lichens(symbiotic relation)\n\n### ice ecosystem\n\n- no lichens\n\n# Biological Terms\n\n| Term           | Expnation                                                                                   |\n| -------------- | ------------------------------------------------------------------------------------------- |\n| earth zones    | Tropicas, Temperates and Poles                                                              |\n| warm-blooded   | can regulate their body temperature to adjust                                               |\n| cold-blooded   | cannot regulate their body temperature                                                      |\n| arbodial       | animals that can climb                                                                      |\n| mammals        | warm blooded, vertebrates, **mammary glands(milk producing)**, give live births(no eggs)    |\n| reptiles       | arbodial, cold-blooded, no mammary glands, presence of lungs                                |\n| insects        | non-vertebrates                                                                             |\n| birds          | warm-blooded, feather, can fly                                                              |\n| mangroves      | grow in wet areas, **some roots above the ground**, only beings that can grow in salt water |\n| fish           | verterbrates(spinal cord)                                                                   |\n| brackish water | mix of salt and freshwater(inter-tidal zones)                                               |\n| pneumatophores | roots that can take nutirents from atmosphere                                               |\n|                |                                                                                             |\n\n## importance of mangronves\n\n- need both salt and fresh water\n- can extract fresh water from salt water\n  - need for fresh is a problem nowadays\n- hold the coast line\n- mangroves are in danger because of the port industry\n- very high capacity(4x more than terrestial) of absorbing carbod dioxide \n","n":0.046}}},{"i":49,"$":{"0":{"v":"Lec 1","n":0.707},"1":{"v":"## environment vs ecology\n\n- env: everything in our surrounding\n- eco: study of interaction among different living beings\n\n## biosphere\n\n- region of surface that supports life\n- sum of all ecosystems\n\n## ecosystem\n\n<blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">all organism with the physical environment they interact with</blockquote>\n\n### types\n\n#### aquatic\n\n- salt water / marine \n  - Ex: oceans\n- fresh water \n  - Ex: rivers\n  \n#### tundra (exteme cold climate)\n\n#### terrestial\n\n- forest\n- desert \n","n":0.116}}},{"i":50,"$":{"0":{"v":"Computer Vision","n":0.707}}},{"i":51,"$":{"0":{"v":"Lec 9","n":0.707},"1":{"v":"\n## Second moments of a region\n\n- to find the parameters $\\lambda_1$ and $\\lambda_2$, we use *second moments*\n- we take a window $W$ centered at a pixel, and compute 3 parameters\n  - $a = \\sum_{i\\in W} (I_{x_i})^2$ - sum of intensity in x-direction\n  - $b = 2 \\sum_{i\\in W} (I_{x_i}^2 * I_{y_i}^2)$ - sum of product of intensities in x and y direction\n  - $c = \\sum_{i\\in W} (I_{y_i})^2$ - sum of intensity in y-direction\n- The length of the parameters, semi-major axis $\\lambda_1$ and semi-min axis $\\lambda_2$ can be calculated as:\n  - $\\lambda_1 = E_{max} = \\frac{1}{2} [a + c + \\sqrt{b^2 + c^2 + (a-c)^2}]$\n  - $\\lambda_2 = E_{min} = \\frac{1}{2} [a + c - \\sqrt{b^2 + c^2 + (a-c)^2}]$\n- based on the values of $\\lambda_1$ and $\\lambda_2$, we can classify into\n  - $\\lambda_1 \\backsim \\lambda_2,\\; both \\; are \\; small \\implies flat\\hspace{1mm}region$\n  - $\\lambda_1 >> \\lambda_2 \\implies edge\\hspace{1mm}region$\n  - $\\lambda_1 \\backsim \\lambda_2,\\; both \\; are \\; large \\implies corner\\hspace{1mm}region$\n\n### Harris Corner Detection\n\n![](/assets/images/2022-02-08-17-34-46.png)\n- plotting the $\\lambda_1 vs \\lambda_2$ distribution, we can find the regions for different types of surfaces\n\n<br>\n\n- Harris defines $R = \\lambda_1 \\lambda_2 - K(\\lambda_1 + \\lambda_2)^2$, $0.04 < K 0.05$\n- Thus we get a single boundary for detecting corners\n\n#### Non Maximal Supression\n\n![](/assets/images/2022-02-08-21-15-19.png)\n- after detecting the cornes using a single threshold, we generally get a cluster of corners\n- we use non-maximal supression to reduce false positives\n  - we slide a *window* k over the image\n  - at each position, if the pixel at the center is not maximum we supress it and only reatin those which are maximum\n  \n# Boundary Detection\n\n- fit the object boundaries from the edge pixels\n  - fit lines or curves to the edges\n  - active contours(*snakes*)\n  - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">Hough Transform</code>\n- some preprocessing is performed to improve the quality\n  - i.e. filtering out some oultiers or noisy pixels\n\n<details>\n<summary>preprocessing process</summary>\n\n![](/assets/images/2022-02-08-14-04-15.png)\n\n</details>\n\n## Fitting lines and curves to edges\n\n![](/assets/images/2022-02-08-21-24-37.png)\n- we define the error function(veritcal distance) as\n- $\\frac{1}{n}\\sum_i (y_i - m*x_i - c)^2$\n- for minmizing the error, we take partial derivatives with respect to m and c, and set them to 0\n- $m = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i (x_i - \\bar{x})^2}, \\hspace{1mm} c = \\bar{y} - m\\bar{x}$\n\n# Extendel Learning / TODO\n\n- [ ] If the points are oriented vertically, how do we find the vertical distance\n","n":0.051}}},{"i":52,"$":{"0":{"v":"Lec 8","n":0.707},"1":{"v":"\n## Canny edge detection steps in detail\n\n- smoothing with 2D gaussain filter - $n_{\\sigma} * I$\n- compute image gradient using *sobel* operator - $\\nabla n_{\\sigma} * I$\n\n<details>\n<summary>find the magnitude and direction of the gradient</summary>\n\n![](/assets/images/2022-02-08-23-31-51.png)\n- brighter the point, higher the magnitude\n- direction is given by the green arrow\n\n</details>\n\n<details>\n<summary>compute the <i>laplacian</i> along the direction of gradient</summary>\n\n![](/assets/images/2022-02-08-23-34-58.png)\n- instead of the normal 2D laplacian, canny used a 1D laplacian along the direction of the gradient\n\n</details>\n\n- apply non maximum supression to remove false positives\n- apply hysteresis threshold\n\n<details>\n<summary>canny edge detection results</summary>\n\n![](/assets/images/2022-02-08-23-44-44.png)\n\n</details>\n\n# Corner \n\n![](/assets/images/2022-02-03-13-53-42.png)\n- rapid change of intensity in two direction within a small region or meeting of two edges\n\n## Corner Detection\n\n<details>\n<summary>gradients for flat, edge and corner region</summary>\n\n![](/assets/images/2022-02-03-14-04-14.png)\n\n</details>\n\n- plotting the distribution of $I_x$ and $I_y$, we get\n- ![](/assets/images/2022-02-03-14-16-17.png)\n  - these elliptical distributions can be *parmaterized* based on the *semi-minor axis* and *semi-major axis* of the ellipses\n    - $\\lambda_1: semi\\,major\\,axis$\n    - $\\lambda_2: semi\\,minor\\,axis$\n\n# TODO\n\n- [ ] non maximal supression, hystersis threshold and its code ","n":0.08}}},{"i":53,"$":{"0":{"v":"Lec 7","n":0.707},"1":{"v":"\n# Gaussian Smoothing\n\n<details>\n<summary>smoothing using gaussian convolution</summary>\n\n![](/assets/images/2022-02-08-21-52-43.png)\n\n</details>\n\n- we use gaussain smoothing to supress the noice by convolving the gaussian\n- to save computation power, instead of first smoothening out image and then finding the derivate to find edge, \n  - we apply the differentiation on the gaussain filter and then applied the combined filter on the image\n  - *this is because the convolution function associativity*\n  - this is knows as <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">LoG</code> (*laplacian of gaussian*)\n- *PRINCIPLE*: gaussian assign a high value to the central input and also considers the **neighboring** inputs\n\n<details>\n<summary>gradient vs laplacian</summary>\n\n![](/assets/images/2022-02-08-22-10-39.png)\n![](/assets/images/2022-02-08-22-11-00.png)\n\n</details>\n\n# Canny Edge Detection\n\n- combining good points from sobel and laplacian\n\n## objective\n\n- good detection: true edges, and minimal false positives\n- good localisation: sub pixel level accuracy\n- single point constrains: only one point corresponding to edge point\n\n## Steps\n\n1. smoothing with 2D gaussain filter\n2. compute image gradient using *sobel* operator\n3. find magnitude and orientation of gradient at each pixel\n4. compute the *laplacian* along the direction of gradient\n5. apply non maximum supression to remove false positives\n6. apply hysteresis threshold","n":0.076}}},{"i":54,"$":{"0":{"v":"Lec 6","n":0.707},"1":{"v":"# Thresholding\n\n- ![](/assets/images/2022-01-30-15-52-25.png)\n- thresholding helps us to get a sharper edge and get a better idea of shape\n- a simple approach would be to fix a bound and perform binary classification(edge, non-edge)\n\n## Hysteresis Thresholding\n\n- if the gradient at a pixel is:\n  - above *high threshold* $T_h$, classify it as **edge**\n  - below *low threshold* $T_l$, classify it as **non-edge**\n  - for any pixel in between\n    - iteratively classify based on the neighbors\n\n### Implementation\n\n- $T_h:T_l = 1:2\\hspace{1mm}or1:3$\n- select $T_h$ such that edges cannot be ignored - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">seed points</code>\n- from the seed move to the *8-neighborhood* as long as the pixels does not fall below $T_l$\n\n# Edge Detection using Second Derivative\n\n<details>\n<summary>edge derivative plots</summary>\n\n![](/assets/images/2022-02-01-12-03-57.png)\n\n</details>\n\n- first derivative gives us the strength and direction\n- second derivative is used to detect *polarity*\n- *zero crossing* indicates the presence of edge and can locate thick edges\n\n## Discrete lapacian operator\n\n- finite differences of second order are given given by the <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">del square</code> ($\\nabla_2 I$)\n<details>\n<summary>formulae for del-square</summary>\n\n![](/assets/images/2022-02-01-16-03-54.png)\n\n</details>\n\n### Implementation\n\n- ![](/assets/images/2022-02-01-16-05-17.png)\n- this can be implemented using the convolution masks:\n- ![](/assets/images/2022-02-01-16-28-48.png)\n- when we appy this convolution masks, the convoluted matrix contains negative values\n  - when plotting this image without normalising, we get an image where negative values give a blackish pixels, 0 gives greyish pixel and positive values gives brighter pixels\n  - therefore before plotting the final image, we need to normalize it to get a prope image\n\n# Effect of noise\n\n![](/assets/images/2022-02-01-12-19-45.png)\n- if the noise increases, it becomes difficult to differentiate between noise and edge as the derivate detects rapid change\n\n<br>\n\n- noise in the image is almost invisible \n- but *difference filters* respond very strongly to noise\n- noise changes the neighborhood and the outcome is very different\n- solutions\n  - if the frequency is high, remove using low pass filter\n  - use kernels of higher size to decrease noise sensitivity","n":0.057}}},{"i":55,"$":{"0":{"v":"Lec 5","n":0.707},"1":{"v":"# Edge Detection using Gradients\n\n<details>\n<summary>gradient plots</summary>\n\n![](/assets/images/2022-01-30-11-22-19.png)\n\n</details>\n\n- we want to detect *rapid change* in intensity, therefore we find the gradient of the image \n- local maximas $\\implies$ position of the edges \n- height of the local $\\implies$ maxima strength of the edges\n- for detecting edges in a 2d space, we use *partial derivate* \n$$\n\\nabla I = [\\frac{\\delta I}{\\delta x}, \\frac{\\delta I}{\\delta x}]\n$$\n- *gradient magnitue*: $S=\\sqrt{(\\frac{\\delta I}{\\delta x})^2 + (\\frac{\\delta I}{\\delta x})^2}$ and *gradient direction*: $\\theta = tan^{-1}(\\frac{\\delta I}{\\delta x} / \\frac{\\delta I}{\\delta x})$\n\n## Finite differences for matrices\n\n- as the image has discrete intensity values for pixel, we find the finite differences \n- ![](//assets/images/2022-01-30-15-36-15.png)\n- this operations can be expressed in the form of <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">kernels</code>\n\n### Kernel examples\n\n![](/assets/images/2022-01-30-15-39-17.png)\n<details>\n<summary>smoothing vs localisation example</summary>\n\n![](/assets/images/2022-01-30-15-46-09.png)\n\n</details>\n\n- **_As the size of kernel increases, smoothness increases and localisation decreases, noise sensitivity decreases_**","n":0.085}}},{"i":56,"$":{"0":{"v":"Lec 4","n":0.707},"1":{"v":"# Edge Detection\n\n- identifying discontinuity in an image\n- to get better idea about the shape and semantics\n- provides more information than pixel level\n\n## Edge Formation\n\n- rapid change in intensity\n- can be caused by:\n  - ![](/assets/images/2022-01-30-10-37-43.png)\n  - intensity or color change\n  - surface normal discontinuity\n  - depth discontinuity\n  - surface color discontinuity\n  - illumination discontinuity\n\n## Edge Types\n\n![](/assets/images/2022-01-30-10-45-04.png)\n- **_due to sampling and quantization, real edges are noisy_**\n\n## Edge Detector and Performance\n\n- We need edge operator to produce:\n  - Edge position\n    - sub pixel level accuracy\n  - Edge magnitude – Strength\n  - Edge Orientation – Direction\n- Performance measurement\n  - High detection rate\n  - Good localization\n  - Low noise sensitivity","n":0.097}}},{"i":57,"$":{"0":{"v":"Lec 3","n":0.707},"1":{"v":"\n# Camera Calibration\n\n- we take an object of known geometry and its image map the correspoding points\n![](/assets/images/2022-01-27-19-04-05.png)\n- mapping more points and rearrangin into a matrix we get:\n![](/assets/images/2022-01-27-19-06-21.png)\n- thus we get an equation of the form $AP = 0$\n- we are in the *homogeneous system*, and scaling the coordinates gives us the same *cartesian coordinates*\n  - thus we can set $\\lvert\\lvert P \\rvert\\rvert^2$ to any arbitrary value\n  - we set $\\lvert\\lvert P \\rvert\\rvert^2 = 1$ for mathematical convenience \n  \n## Constrained Least  Squares\n\n- thus we need to find the solution for $AP = 0$ such that $\\lvert\\lvert P \\rvert\\rvert^2 = 1$\n  - this problem is known as <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">constrained least-squares</code>\n  - i.e. $\\begin{matrix}min\\\\ p\\end{matrix} \\lvert\\lvert AP \\rvert\\rvert^2$ such that $\\lvert\\lvert P \\rvert\\rvert^2 = 1$\n- to find best approximate solution, we try to minimize the loss function \n$$\nl(P, \\lambda) = (AP)^TAP - \\lambda(P^TP-1)\n$$\n- taking the derivate and setting to $0$ to minimize, we get \n$$\n2AP - 2\\lambda P = 0 \\implies \\boxed{AP = \\lambda P}\n$$\n- **_Eigen vector corresponding to smallest eigenvalue gives us P_**\n\n## Finding the intrinsic and extrinsic parameters\n\n- using the properties of calibration matrix and rotation matrix, we can find them using the *QR decomposition* of the first 3 cols of *projection matrix* P \n- ![](/assets/images/2022-01-27-19-38-51.png)\n","n":0.069}}},{"i":58,"$":{"0":{"v":"Lec 2","n":0.707},"1":{"v":"\n[Homogeneous Coordinates](http://www.songho.ca/math/homogeneous/homogeneous.html)\n\n## Using Homogeneous Coordinates to get a linear equation\n![[dendron:Lec 1|acads.sem-6.computer-vision.lec-1#^dAINqR5UIUcW]]\n- to remove the non-linearity in the above equations, we use the homogeneous coordinates of the image point $(u, v)$\n  - we bump the dimension using scalar $z_c$ to get rid of the denominator\n$$\n\\begin{pmatrix}\n\\hat{u} \\\\ \\hat{v} \\\\ \\hat{w} \n\\end{pmatrix} \\equiv \n\\begin{pmatrix}\n\\hat{z_cu} \\\\ \\hat{z_cv} \\\\ \\hat{z_c} \n\\end{pmatrix} =\n\\begin{pmatrix}\n{f_xx_c + o_xz_c} \\\\ {f_yy_c + o_yz_c} \\\\ {z_c} \n\\end{pmatrix}\n$$\n- to capture the transformation from camera coordinate $[x_c\\hspace{2mm}y_c\\hspace{2mm}z_c\\hspace{2mm}1]$ to the image coordinate $[\\hat{u}\\hspace{2mm}\\hat{v}\\hspace{2mm}\\hat{w}]$, we use a single 3x4 matrix\n$$\n\\begin{pmatrix}\n\\hat{u} \\\\ \\hat{v} \\\\ \\hat{w} \n\\end{pmatrix} = \n\\begin{pmatrix}\nf_x&0&o_x&0 \\\\ 0&f_y&o_y&0 \\\\ 0&0&1&0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_c\\\\y_c\\\\z_c\\\\1\n\\end{pmatrix}\n$$\n- this matrix is known as matrix of intrinsic properties $M_{int}$ and the inner 3x3 matrix is knows as *calibration matrix* $K$\n  - i.e. $M = [K\\hspace{2mm}0]$ \n  - **_K is an upper triangular matrix_**\n\n# World Coordinate to Camera Coordinate\n\n- this tranfomration is a change of coordinate system and can be characterized by an **_orthonormal_** rotation matrix $R$ and a translation vector $\\overrightarrow{t}$\n$$\n\\begin{pmatrix}\nx_c\\\\y_c\\\\z_c\n\\end{pmatrix} = \n\\begin{pmatrix}\nr_{11} & r_{12}&r_{13}\\\\ r_{21}&r_{22}&r_{23}\\\\ r_{31}&r_{32}&r_{33}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_w\\\\y_w\\\\z_w\n\\end{pmatrix} + \n\\begin{pmatrix}\nt_x\\\\t_y\\\\t_z\n\\end{pmatrix} \n$$\n- this can be rewritten in homogeneous system as\n$$\n\\begin{pmatrix}\nx_c\\\\y_c\\\\z_c\\\\1\n\\end{pmatrix} = \n\\begin{pmatrix}\nr_{11} & r_{12}&r_{13}&t_x\\\\ r_{21}&r_{22}&r_{23}&t_y\\\\ r_{31}&r_{32}&r_{33}&t_z\\\\0&0&0&1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_w\\\\y_w\\\\z_w\\\\1\n\\end{pmatrix}  \n$$\n- this in knows as *matrix of external parameters* $M_{ext}$ and $M_{ext} = \\begin{pmatrix}R&t\\\\0&1\\end{pmatrix}$\n- the projection vector $P$ can be expressed as $P = M_{int}M_{int}$\n","n":0.07}}},{"i":59,"$":{"0":{"v":"Lec 10","n":0.707},"1":{"v":"\n## Fitting a curve to edges\n\n![](/assets/images/2022-02-10-13-39-39.png)\n- we need to find the polynominal which minimizes the perpendicular distance\n- $E = \\frac{1}{N} \\sum_i (y_i - ax_i^3 - bx_i^2 - cx_i - d)$\n  - we solve linear(linear function of parameters) system of equations using *least squares*\n  - we find the *partial derivative* with respect to all the params and set it to zero to find the minima\n\n<br>\n\n- we need to determine the *degree* of the polynomial that best fits the given edges\n  - as the number of params is very less comprared to the number of equation, get a **_over-determined_** linear system\n- rearraning these equation into a matrix form, we get\n- ![](/assets/images/2022-02-10-13-53-14.png)\n- finding the moore penrose *pseduo-inverse* for this system of equation, we get the least squares solution\n- ![](/assets/images/2022-02-10-13-59-30.png)\n\n# Active Contours(Snakes)\n\n![](/assets/images/2022-02-10-14-04-21.png)\n- iterativel deforms initial contours unitl:\n  - it is near the pixel with *high gradients* (edges)\n  - it is smooth\n\n## Representing a contour\n\n![](/assets/images/2022-02-10-14-10-25.png)\n- contour is described using a ordered list of 2d vertices(control points)\n- lines connecting the *control points*\n\n## Attracting contours to edges\n\n![](/assets/images/2022-02-10-14-18-51.png)\n- we attract the contour by maximizing the sum of *gradient magnitude square*\n- this is process is knows as <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">boundary tracking</code>","n":0.071}}},{"i":60,"$":{"0":{"v":"Lec 1","n":0.707},"1":{"v":"# Computer Vision\n\n- ![computer vision](/assets/images/2022-01-20-11-22-17.png)\n- light(simple point source or any other complex sytem) falls on a scene and gets scattered and reflected\n- camera captures this light by digitalizing the light intensity\n  - *sampling* and *quantization* are involved\n  - image is then represented as matrix of these values (*pixels*)\n- a computer vision derives some information from this and gives scence description\n\n# Image formation model\n\n- how a 3d object is projected onto a 2d image plane\n- we understand this to later reconstruct some information from a 2d image\n\n## Perspective projection with Pinhole\n\n![](/assets/images/2022-01-25-10-21-43.png)\n- the image of a point $P_o$ with position vector $\\overrightarrow{r_o} = (x_o, y_o, z_o)$ gets projected on the image plane at point $P_i$ with position vector $\\overrightarrow{r_i} = (x_i, y_i, f)$\n- from the similarities of triangle we get:\n$$\n\\frac{\\overrightarrow{r_i}}{f} = \\frac{\\overrightarrow{r_o}}{z_o} \\implies \\frac{x_i}{f} = \\frac{x_o}{z_o}\\hspace{2mm},\\hspace{2mm} \\frac{y_i}{f} = \\frac{y_o}{z_o}\n$$\n- these equations are known as *equations of perspective projection*\n\n### Properties of perspective projection\n\n- straign line in the scene remains a straight line in the image\n- maginification $m$ is give by $m = \\frac{f}{z_o}$\n  - reason for point intersecting at infinity\n\n## Linear Camera Model\n\n- mapping the 3D points into 2D image plane\n- also known as *forward imaging model*\n![](/assets/images/2022-01-25-12-11-10.png)\n- the image formed depends on:\n  - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">Extrinsic Parameter</code> - orientation of camera with respect to the object in the 3D world (*world coordinate frame*)\n  - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">Intrinsic Parameter</code> - how the camera maps the 3D coordinates of the object on an 2D image plan(internal parameters of the camera)\n- in the forward imaging model: \n  - ![](/assets/images/2022-01-25-12-22-12.png)\n  - world coordinates of a point $X_w = [x_w y_w z_w]^T$(**_position vector with respect to the world coordinate system_**) gets transformed to camera coordinates $X_c = [x_c y_c z_c]^T$(**_position vector with respect to the camera coordinate system_**)\n  - this camera coordinates get mapped on the image plane $X_i = [x_i, y_i]^T$ using the perspective projection\n\n### Camera to image transformation\n\n- using the *equations of perspective projection*, we can find the relation between image coordinates and the camera coordinates\n$$\n\\frac{x_i}{f} = \\frac{x_c}{z_c}\\hspace{2mm},\\hspace{2mm} \\frac{y_i}{f} = \\frac{y_c}{z_c}\n$$\n\n#### Image plane to image matrix\n\n![](/assets/images/2022-01-25-17-06-05.png)\n- the image plane is continuous, but the matrix give by the light sensors is discrete\n- if we consider the *density*(pixels/mm) of pixels in the  $\\hat{x}$ and $\\hat{y}$ direction as $m_x$ and $m_y$ and number of pizels be $u$ and $v$, then we can relate them by:\n$$\nu = m_x x_i \\hspace{2mm},\\hspace{2mm} v = m_y y_i\n$$\n- using the equations between image and camera corrdiantes, we get\n$$\nu = m_x f \\frac{x_c}{z_c} \\hspace{2mm},\\hspace{2mm} v = m_y f \\frac{y_c}{z_c}\n$$\n- the origin of the image sensor may be at the center, thus we consider a <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">principle point</code> with the pixel $(o_x, o_y)$ and the final equation is given by:\n$$ \nu = f_x \\frac{x_c}{z_c} + o_x \\hspace{2mm},\\hspace{2mm} v = f_y \\frac{y_c}{z_c} + o_y\n$$\n^dAINqR5UIUcW\n- $f_x$ and $f_y$ are the effective focal length in pixels, in the $\\hat{x}$ and $\\hat{y}$ direction\n","n":0.045}}},{"i":61,"$":{"0":{"v":"Deep Learning","n":0.707},"1":{"v":"\n# Beyond Linear Mapping\n\n- a simple way to increase flexibility -> *Feature Transformation*\n  - still linear in term of params -> fitting easy\n  - but manual feature transformation is very limiting and requires very strong domain knowledge\n\n- modelling the feature extactor to increase flexibility\n$$\nf(x, \\theta) = W\\phi(x;\\theta_{2}) + b\n$$\n- $\\theta = (\\theta_1, \\theta_2)$ and $\\theta_1 = (W, b)$\n- reursively repeating the process to create complex functions\n$$\nf(x, \\theta) = f_L(f_{L-1}(...f_1(x)...))\n$$\n  - key idea behind deep neural networks\n  - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">DNN</code>\n    - mapping input to output using DAG(directed acyclic graph) of functions\n    - also known as **_feed forward network_** or **_multi layer perceptron_**\n    - CNN for images, RNN for sequences, GNN for graphs\n\n# Multi Layer Perceptron\n\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">perceptron</code> - heavyside step function or linear threshold function\n$$\nf(x;\\theta) = I(w^T + b \\geq 0) = H(w^T + b)\n$$\n\n- as decision boundaries are linear, represntation is very limiting\n  \n<details>\n<summary>for simple XOR, we need multiple layers</summary>\n\n![](/assets/images/2022-05-03-17-27-14.png)\n\n</details>\n\n## Importance of depth\n\n- depth helps in hierarchial feature extraction\n- deeper layers can leverage the learning of previous layers\n\n# ANN and Neuron Structure\n\n![](/assets/images/2022-05-08-08-08-26.png)\n\n# Learning\n\n## Gradient Descent\n\n- iterative optimization algo for finding minima\n- **_idea_**: repeated steps in the direction opposite to the graident at current point\n\n## Batch gradient descent\n\n```python\nweights = initialize_random_weights(shape)\n\nfor _ in range(epochs):\n  average_gradient = find_average_gradient(data, loss_fn, weights)\n  weights = weights - learning_rate * average_gradient\n```\n\n## Stochastic gradient descent\n\n```python\nfor _ in range(epochs):\n  data = shuffle_data(data)\n  for row in data:\n    row_gradient = find_row_gradient(row, loss_fn, weights)\n    weights = weights - learning_rate * row_gradient\n```\n\n## Mini-batch graident descent\n\n```python\nfor _ in range(epochs):\n  data = shuffle_data(data)\n  for batch in get_batches(data, batch_size):\n    batch_gradient - find_batch_gradient(batch, loss_fn, weights)\n    weights = weights - learning_rate * batch_gradient\n```\n\n<details>\n<summary><b><i>Comparison</i></b></summary>\n\n### Loss Plots\n![](/assets/images/2022-05-08-10-56-40.png)\n\n### Properties\n![](/assets/images/2022-05-08-10-57-13.png)\n\n</details>\n\n\n## Problems in Descent\n\n1. learning rate pace\n   - learning rate low -> slow convergence\n   - learning rate high -> fluctuations and overfitting\n\n2. imbalanced data\n   - if the data has features with different frequencies\n     - cannot apply same learning to all features\n     - need high learning rate for rare features\n   - loss changes quickly in one direction and slowly in other\n\n3. non-convex loss functions\n   - suboptimal **local minimas**\n   - **saddle points**(one dimension slopes up another slopes down)\n      - surrouded by plateau of same error, difficult to escape\n   - **cliffs** in RNN\n\n4. NNs\n   - vanishing and exploding gradients\n\n## Solutions\n\n### Momentum\n\n- helps to accelerate descent in relevant\n- reduces oscillations\n- **_idea:_** take a fraction of past updates\n  - momentum at time t: $v_t = \\gamma v_{t-1} + \\eta \\frac{\\delta J(W)}{\\delta W}$\n  - Update: $W = W - v_t$\n  \n<details>\n<summary>explanation</summary>\n\n![](/assets/images/2022-05-08-12-04-59.png)\n\n- consider the following example\n  - here the gradient in vertical direction is oscillating\n  - in horiontal direction is approaching optimum\n- if we take the sum of previous gradients:\n  - oscillation in vertical direction will cancel out\n  - velocity in horizontal direction builds up\n\n</details>\n\n### Neterov Accelerate Gradient\n\n- prevents us from going too fast and overshoot\n- **_idea:_** take gradient at future value for calculating momentum\n  - momentum at time t: $v_t = \\gamma v_{t-1} + \\eta \\frac{\\delta J(W - \\gamma v_{t-1})}{\\delta W}$\n  - Update: $W = W - v_t$\n\n<details>\n<summary>illustration</summary>\n\n![](/assets/images/2022-05-08-12-16-46.png)\n\n</details>\n\n### AdaGrad\n\n- $S = \\sum \\frac{\\delta J(W)}{\\delta W}$\n- Update: $W = W - \\eta \\frac{\\delta J(W)}{\\delta W} * \\frac{1}{\\sqrt{S + \\epsilon}}$\n  - over a batch / mini-batch  \n- makes learning slow for predictors with high gradient\n- makes learning fast for predictors with low gradient\n\n### Regularization\n\n- to avoid overfitting, punish the complexity\n- adding a regularization term to the objective / loss function\n\n![](/assets/images/2022-05-08-16-50-09.png)\n\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">L2 regularization</code> -> $R(W) = \\sum\\sum W_{k, l}^2$\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">L1 regularization</code> -> $R(W) = \\sum\\sum | W_{k, l} |$\n- <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">L1 + L2 regularization</code> -> $R(W) = \\sum\\sum \\beta W_{k, l}^2 + | W_{k, l} |$\n\n### Dropouts\n\n- standard practice: dropout 50% of the neurons\n- at each epoch the random 50% changes and thus, the chance of a neruons overfitting decreases\n\n# Backpropogation\n\n- [ ] [Backpropagation Step by Step](https://hmkcode.com/ai/backpropagation-step-by-step/)\n# Activation Functions\n\n- purpose -> to introduce non-linearity in the network\n  - using a linear function will just give us a line in n-dimension\n  - ![](/assets/images/2022-05-08-07-50-23.png)\n\n## Some common activation functions\n  \n### Sigmoid\n\n- smoothing of heaviside function\n- between 0 and 1\n- gets saturated at extreme values\n\n### tanh\n\n- similar nature as sigmoid\n- between -1 and 1\n\n### ReLU\n\n- piecewise linear\n- solves the problem of vanishing graidents in sigmoid and tanh\n\n\n# Automatic Differentiation and Computational Graphs\n\n- dl frameworks provide *autograd*\n- mainatain a <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">computation graph</code>\n  - directed graphs describing the flow of data\n  - breaks the functional chains in the form of a graph\n  - [ ] [How Computational Graphs are Constructed in PyTorch](https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/)\n    - [ ] [Pytorch Autograd](https://pytorch.org/blog/overview-of-pytorch-autograd-engine/)\n    - [ ] [Understanding accumulated gradients in PyTorch](https://stackoverflow.com/questions/62067400/understanding-accumulated-gradients-in-pytorch)","n":0.036}}},{"i":62,"$":{"0":{"v":"Computer Networks","n":0.707}}},{"i":63,"$":{"0":{"v":"Big Data Analytics","n":0.577}}},{"i":64,"$":{"0":{"v":"Lec 1","n":0.707}}}]}
