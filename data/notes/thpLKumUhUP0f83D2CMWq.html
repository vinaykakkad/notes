<h1 id="8-mle-2">8-MLE-2<a aria-hidden="true" class="anchor-heading icon-link" href="#8-mle-2"></a></h1>
<h1 id="model-fitting-or-training">Model Fitting or Training<a aria-hidden="true" class="anchor-heading icon-link" href="#model-fitting-or-training"></a></h1>
<ul>
<li>estimating <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span></span> from a given data
<ul>
<li>we estimate it by minimizing the loss function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span></li>
</ul>
</li>
</ul>
<h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation<a aria-hidden="true" class="anchor-heading icon-link" href="#maximum-likelihood-estimation"></a></h2>
<ul>
<li>we pick the parameters that assign highest probability to the training data</li>
<li>we define MLE as</li>
<li><img src="/notes/assets/images/2022-02-07-11-43-45.png"></li>
<li>we assume that the training examples are independently sampled from the same distribution, ie <code style="background-color: #43b02a40; padding:3px 2px; border-radius: 5px">iid</code>(indpendent and identically distributed)</li>
<li>the conditional likelihood then becomes</li>
<li><img src="/notes/assets/images/2022-02-07-11-50-56.png">
<ul>
<li><strong><em>as the examples are independent, the overall probability can be expressed as the product of individual probabilities</em></strong></li>
</ul>
</li>
<li>We use a log likelihood:</li>
<li><img src="/notes/assets/images/2022-02-07-11-54-31.png">
<ul>
<li>it is mathematically convenient to use log function as the likelihood can be expessed as summation</li>
<li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">log</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span></span> is a monotonically increasing function and provides one to one mapping</li>
</ul>
</li>
<li>The <em>MLE</em> is then given by:</li>
<li><img src="/notes/assets/images/2022-02-07-11-56-51.png"></li>
</ul>
<br>
<ul>
<li>most of the cost functions are designed to <em>minimize</em> const function, we can redfine objective function to be a <em>negative log likelihood</em> or <code style="background-color: #43b02a40; padding:3px 2px; border-radius: 5px">NLL</code></li>
<li><img src="/notes/assets/images/2022-02-07-11-58-41.png"></li>
<li>minimizing this gives us the <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><msub><mi>θ</mi><mrow><mi>m</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{\theta_{mle}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1079em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
</ul>
<h4 id="optimal-theta-for-unsupervisedunconditional-models">Optimal theta for unsupervised(unconditional) models<a aria-hidden="true" class="anchor-heading icon-link" href="#optimal-theta-for-unsupervisedunconditional-models"></a></h4>
<ul>
<li>as we only have outputs and no inputs in this case, MLE becomes</li>
<li><img src="/notes/assets/images/2022-02-08-08-02-15.png"></li>
<li>alternatively, we can maximize the <em>join likelihood</em> of inputs and outputs. The MLE in this case becomes</li>
<li><img src="/notes/assets/images/2022-02-08-08-07-39.png"></li>
</ul>
<h3 id="example-mle-for-bernoulli-distribution">Example: MLE for Bernoulli distribution<a aria-hidden="true" class="anchor-heading icon-link" href="#example-mle-for-bernoulli-distribution"></a></h3>
<ul>
<li>Y is <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>V</mi></mrow><annotation encoding="application/x-tex">RV</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span> representing a coin toss
<ul>
<li>Y = 1 corresponds to heads</li>
<li>Y = 0 cooresponds to tails</li>
</ul>
</li>
<li>Let <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\theta = p(Y=1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span> be the probability of heads</li>
<li>MLE for such a <em>bernoulli dist.</em> can be given by:</li>
<li><img src="/notes/assets/images/2022-02-08-08-19-06.png"></li>
<li>finding the derviative and setting it to 0, we get</li>
<li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><msub><mi>θ</mi><mrow><mi>m</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>^</mo></mover><mo>=</mo><mfrac><msub><mi>N</mi><mi>h</mi></msub><mrow><msub><mi>N</mi><mi>h</mi></msub><mo>+</mo><msub><mi>N</mi><mi>t</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\hat{\theta_{mle}} = \frac{N_h}{N_h + N_t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1079em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3451em;vertical-align:-0.4509em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8942em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.109em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em;"><span style="top:-2.357em;margin-left:-0.109em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4159em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.109em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4509em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>, which is similar to intuitive result</li>
</ul>
<br>
<ul>
<li><strong>Here we have not incoporated <em>population risk</em> and thus there is a high change of overfitting the training set</strong>
<ul>
<li>The model in this case has enough parameters to perfectly fit the observed training data</li>
</ul>
</li>
</ul>