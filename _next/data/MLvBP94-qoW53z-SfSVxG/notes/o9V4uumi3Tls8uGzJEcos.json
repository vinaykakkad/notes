{"pageProps":{"note":{"id":"o9V4uumi3Tls8uGzJEcos","title":"10 Bayesian Statistics","desc":"","updated":1644951005537,"created":1644818744038,"custom":{},"fname":"acads.sem-6.machine-learning.10-bayesian-statistics","type":"note","vault":{"fsPath":"vault"},"contentHash":"bc3233d19cd6151fde0365a8efcfb7e5","links":[],"anchors":{"approaches-for-machine-learning":{"type":"header","text":"Approaches for Machine Learning","value":"approaches-for-machine-learning","line":7,"column":0,"depth":2},"weight-decay":{"type":"header","text":"Weight Decay","value":"weight-decay","line":18,"column":0,"depth":2},"finding-the-optimal-regularization-parameter":{"type":"header","text":"Finding the optimal regularization parameter","value":"finding-the-optimal-regularization-parameter","line":33,"column":0,"depth":2},"todo":{"type":"header","text":"TODO","value":"todo","line":44,"column":0,"depth":1}},"children":[],"parent":"TN2l8VL0nEIPfVAADRxm5","data":{}},"body":"<h1 id=\"10-bayesian-statistics\">10 Bayesian Statistics<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#10-bayesian-statistics\"></a></h1>\n<h2 id=\"approaches-for-machine-learning\">Approaches for Machine Learning<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#approaches-for-machine-learning\"></a></h2>\n<ul>\n<li><em>frequentist</em> (<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>D</mi><mo separator=\"true\">;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(D;\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span>)\n<ul>\n<li>find such a <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span> that maximizes that given data</li>\n<li>this approach does not consider <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span> as a random variable\n<ul>\n<li>i.e. the value of parameters does not depend on any prior event</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><em>bayesian statistics</em> (<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>D</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(D|\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span>)\n<ul>\n<li>in this approach the value of params depends on prior knowledge\n<ul>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span> can be encoded as a distribution\n<ul>\n<li>i.e. prior knowledge can be incoporated in the distribution</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"weight-decay\">Weight Decay<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#weight-decay\"></a></h2>\n<ul>\n<li>using regression with very high degree of polynomial results in overfitting\n<ul>\n<li>we tried to reduce the degree of polynomial to generalize the model</li>\n</ul>\n</li>\n<li>another solutions is to penalize the magnitude of weights(<em>regression coefficients</em>)</li>\n</ul>\n<br>\n<ul>\n<li>we use a <em>zero-mean Gaussian</em> prior to penalize the weights, and the resulting MAP is given by</li>\n<li><img src=\"/notes/assets/images/2022-02-16-00-09-17.png\">\n<ul>\n<li>here we use <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span> instead of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span>, since we only penalize the weight vector and not bias terms or noise variances</li>\n</ul>\n</li>\n<li>this is called <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">l<sub>2</sub>regularization</code> or <strong><em>weight decay</em></strong></li>\n<li>in case of linear regression, this kind of penalization scheme is called <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">ridge regression</code></li>\n</ul>\n<h2 id=\"finding-the-optimal-regularization-parameter\">Finding the optimal regularization parameter<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#finding-the-optimal-regularization-parameter\"></a></h2>\n<ul>\n<li>picking a small <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext></mrow><annotation encoding=\"application/x-tex\">\\lambda \\implies</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7184em;vertical-align:-0.024em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">⟹</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span></span></span></span> focussing on minimising empirical risk <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext></mrow><annotation encoding=\"application/x-tex\">\\implies</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.549em;vertical-align:-0.024em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">⟹</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span></span></span></span> <em>overfitting</em></li>\n<li>picking a large <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext></mrow><annotation encoding=\"application/x-tex\">\\lambda \\implies</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7184em;vertical-align:-0.024em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">⟹</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span></span></span></span> focussing on prior <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext></mrow><annotation encoding=\"application/x-tex\">\\implies</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.549em;vertical-align:-0.024em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">⟹</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span></span></span></span> <em>underfitting</em></li>\n<li>we use validation set to find the optimal <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi></mrow><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">λ</span></span></span></span></span></li>\n</ul>\n<p><img src=\"/notes/assets/images/2022-02-15-23-59-38.png\"></p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://github.com/probml/pyprobml/blob/master/scripts/linreg_poly_ridge.py\">code</a></li>\n</ul>\n<h1 id=\"todo\">TODO<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#todo\"></a></h1>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> bias variance dilemma</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> finding validation method for small data\n<ul>\n<li>cross validation</li>\n</ul>\n</li>\n</ul>","noteIndex":{"id":"LBE6GBZjFtOaX27nwRSpq","title":"Notes 📚","desc":"","updated":1642916746120,"created":1642875904363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"f3ab1afac9d88a8f51c98f4e257b81a2","links":[],"anchors":{},"children":["H8OS9Ap2DDjZ7XSIZWpQT","m2fe4910vhlj3cwsbw45wfc","tpmpx2xkasjhbntai7ejfx3","ydBpHsjLwIGYomDZo8NtB","g9m1mlmymq7qvcjzi8ap2qu"],"parent":null,"data":{},"body":"\n![home](/assets/images/home.png)\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.83.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"theme":"custom"},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"assetsPrefix":"/notes","copyAssets":true,"siteHierarchies":["root"],"enableSiteLastModified":true,"siteRootDir":"docs","siteUrl":"https://vinaykakkad.github.io","enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"writeStubs":false,"seo":{"title":"Notes","description":"Notes"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enablePrettyLinks":true,"enableTaskNotes":true,"theme":"custom","searchMode":"lookup","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}