{"pageProps":{"note":{"id":"BmxWSEoOhHxbfsbIbBBSN","title":"2-MLE-and-regression","desc":"","updated":1643001168394,"created":1642921088327,"custom":{},"fname":"acads.sem-6.machine-learning.2-MLE-and-regression","type":"note","vault":{"fsPath":"vault"},"contentHash":"4df8c616952e1031132bdc94cadcd863","links":[],"anchors":{"capturing-uncertainity":{"type":"header","text":"Capturing Uncertainity","value":"capturing-uncertainity","line":7,"column":0,"depth":2},"maximum-likelihood-estimation":{"type":"header","text":"Maximum Likelihood Estimation","value":"maximum-likelihood-estimation","line":21,"column":0,"depth":2},"negative-log-likelihood":{"type":"header","text":"Negative Log Likelihood","value":"negative-log-likelihood","line":27,"column":0,"depth":3},"regression":{"type":"header","text":"Regression","value":"regression","line":37,"column":0,"depth":1},"loss-function":{"type":"header","text":"Loss Function","value":"loss-function","line":41,"column":0,"depth":2},"empirical-risk":{"type":"header","text":"Empirical Risk","value":"empirical-risk","line":48,"column":0,"depth":2},"negative-log-likelihood-1":{"type":"header","text":"Negative Log Likelihood","value":"negative-log-likelihood-1","line":53,"column":0,"depth":3},"capturing-uncertainity-1":{"type":"header","text":"Capturing Uncertainity","value":"capturing-uncertainity-1","line":59,"column":0,"depth":2},"linear-regression":{"type":"header","text":"Linear Regression","value":"linear-regression","line":66,"column":0,"depth":2},"polynomial-regression":{"type":"header","text":"Polynomial Regression","value":"polynomial-regression","line":79,"column":0,"depth":2}},"children":[],"parent":"TN2l8VL0nEIPfVAADRxm5","data":{}},"body":"<h1 id=\"2-mle-and-regression\">2-MLE-and-regression<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#2-mle-and-regression\"></a></h1>\n<h2 id=\"capturing-uncertainity\">Capturing Uncertainity<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#capturing-uncertainity\"></a></h2>\n<ul class=\"contains-task-list\">\n<li>in most of the cases, we can't predict perfectly due to:\n<ul>\n<li><em>intrinsic(irreducible) uncertainity</em>: due to error in data collection...</li>\n<li><em>model uncertainity</em>: due to absence of infinite data / processing power</li>\n</ul>\n</li>\n<li>We capture this uncertainity using conditional probability</li>\n<li><img src=\"/notes/assets/images/2022-01-16-15-52-52.png\" alt=\"capturing uncertainity\"></li>\n<li>here <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo>:</mo><mi>X</mi><mo>→</mo><mo stretchy=\"false\">[</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><msup><mo stretchy=\"false\">]</mo><mi>c</mi></msup></mrow><annotation encoding=\"application/x-tex\">f: X \\rightarrow [0, 1]^c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\"><span class=\"mclose\">]</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6644em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">c</span></span></span></span></span></span></span></span></span></span></span></span> maps the input to one of the <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">c</span></span></span></span></span> labels\n<ul>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span></span></span></span> follows the axioms of probability:\n<ul>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mo>≤</mo><msub><mi>f</mi><mi>c</mi></msub><mo>≤</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0 \\leq f_c \\leq 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7804em;vertical-align:-0.136em;\"></span><span class=\"mord\">0</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">c</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span></li>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msubsup><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>f</mi><mi>c</mi></msub><mo>=</mo><mn>1</mn></mstyle></mrow><annotation encoding=\"application/x-tex\">\\textstyle\\sum_{c=1}^n f_c = 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.104em;vertical-align:-0.2997em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8043em;\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">c</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">c</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span></li>\n</ul>\n</li>\n</ul>\n</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> restriction and <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">softmax function</code></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">affine functions</code> and notations</li>\n</ul>\n<h2 id=\"maximum-likelihood-estimation\">Maximum Likelihood Estimation<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#maximum-likelihood-estimation\"></a></h2>\n<ul class=\"contains-task-list\">\n<li>A common loss function for probabilistic models is <em>negative log probability</em></li>\n<li><img src=\"/notes/assets/images/2022-01-16-16-05-35.png\" alt=\"NLP\"></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> Intution and reason</li>\n</ul>\n<h3 id=\"negative-log-likelihood\">Negative Log Likelihood<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#negative-log-likelihood\"></a></h3>\n<ul>\n<li>Averaging the of training set gives us <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">NLL</code></li>\n<li><img src=\"/notes/assets/images/2022-01-16-16-07-48.png\" alt=\"NLL\"></li>\n</ul>\n<br>\n<ul>\n<li>If we minimize the <em>NLL</em>, we can compute maximum likelihood estimate of <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">MLE</code></li>\n<li><img src=\"/notes/assets/images/2022-01-16-16-08-57.png\" alt=\"MLE\"></li>\n</ul>\n<h1 id=\"regression\">Regression<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#regression\"></a></h1>\n<ul>\n<li><em>response</em> <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span></span> is an continuous, real quantity</li>\n</ul>\n<h2 id=\"loss-function\">Loss Function<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#loss-function\"></a></h2>\n<ul>\n<li>here the loss function is quadratic - <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">l<sub>2</sub> loss</code></li>\n<li><img src=\"/notes/assets/images/2022-01-16-16-11-07.png\" alt=\"quadratic loss\"></li>\n<li>quadratic loss penalizes large <em>residuals</em> <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>−</mo><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">y-\\hat{y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6944em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.1944em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1944em;\"><span></span></span></span></span></span></span></span></span></span> more than small ones \n<ul>\n<li>if the data has outliers, quadratic penalty can be very severe</li>\n<li>in such cases we can use <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">l <sub>1</sub> loss</code></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"empirical-risk\">Empirical Risk<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#empirical-risk\"></a></h2>\n<ul>\n<li>Averraging the quadraticl log function, we get the mean squared error <code style=\"background-color: #43b02a40; padding:3px 2px; border-radius: 5px\">MSE</code></li>\n<li><img src=\"/notes/assets/images/2022-01-16-17-59-02.png\" alt=\"MSE\"></li>\n</ul>\n<h3 id=\"negative-log-likelihood-1\">Negative Log Likelihood<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#negative-log-likelihood-1\"></a></h3>\n<ul>\n<li>Fixing the variance for simplicity and using the <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">l_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0197em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> loss function, we get <em>NLL</em> as:</li>\n<li><img src=\"/notes/assets/images/2022-01-16-18-08-38.png\" alt=\"NLL\"></li>\n<li>this is proportional to MSE\n<ul>\n<li><strong>hence we can calculate maximum likelihood estimate by minmizing the MSE</strong></li>\n</ul>\n</li>\n</ul>\n<h2 id=\"capturing-uncertainity-1\">Capturing Uncertainity<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#capturing-uncertainity-1\"></a></h2>\n<ul>\n<li>In linear regression, output is assumed to <em>Gaussian</em> or <em>Normal</em></li>\n<li><img src=\"/notes/assets/images/2022-01-16-18-02-12.png\" alt=\"Gaussian\"></li>\n<li>Here the mean can be defined using the inputs, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator=\"true\">;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mu = f(x_n;\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">μ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span>, and therefor the probability distribution is given by:</li>\n<li><img src=\"/notes/assets/images/2022-01-16-18-05-09.png\" alt=\"dist\"></li>\n</ul>\n<h2 id=\"linear-regression\">Linear Regression<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#linear-regression\"></a></h2>\n<ul>\n<li>\n<blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">When we fit the data using <b>linear function of the parameters</b>, it is known as linear regression </blockquote>\n</li>\n<li><img src=\"/notes/assets/images/2022-01-23-12-42-44.png\">\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://github.com/probml/pyprobml/blob/master/scripts/linreg_residuals_plot.py\">code</a></li>\n</ul>\n</li>\n<li>we can fit a 1d data using a <em>simple linear regression</em> model of the form</li>\n<li><img src=\"/notes/assets/images/2022-01-23-12-41-37.png\"></li>\n<li>here <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span> is <em>slope</em> and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span></span> is <em>offset</em>, and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>w</mi><mo separator=\"true\">,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\theta=(w,b)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mclose\">)</span></span></span></span></span> are the parameters of the model</li>\n<li>we minimize the squared error to get the least squares solution</li>\n<li><img src=\"/notes/assets/images/2022-01-23-12-39-53.png\"></li>\n<li>If we have multiple <em>predictors</em>, we can have a <strong><em>multiple lineare regression</em></strong></li>\n<li><img src=\"/notes/assets/images/2022-01-23-12-41-00.png\"></li>\n</ul>\n<h2 id=\"polynomial-regression\">Polynomial Regression<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#polynomial-regression\"></a></h2>\n<ul>\n<li>The fitting can be improved by using a <strong><em>polynomial regression</em></strong> model of the form <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">;</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mi>ϕ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(x;w) = w^T\\phi(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">;</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\">ϕ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span></li>\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ϕ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\phi(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">ϕ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span> is a featue vector derived from the input </li>\n<li><img src=\"/notes/assets/images/2022-01-23-12-47-00.png\"></li>\n<li><img src=\"/notes/assets/images/2022-01-23-12-55-41.png\">\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://github.com/probml/pyprobml/blob/master/scripts/linreg_2d_surface_demo.py\">code</a></li>\n</ul>\n</li>\n<li>\n<blockquote style=\"background-color: #43b02a20; padding:3px 2px; border-radius: 5px; border-left: 0.25em solid #43b02a; padding-left: 0.75em\">It is important that the prediction function is a linear function of the parameter because a linear model induces a loss function MSE(θ) that has <b>unique global optimum</b></blockquote>\n</li>\n</ul>","noteIndex":{"id":"LBE6GBZjFtOaX27nwRSpq","title":"Notes 📚","desc":"","updated":1642916746120,"created":1642875904363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"f3ab1afac9d88a8f51c98f4e257b81a2","links":[],"anchors":{},"children":["H8OS9Ap2DDjZ7XSIZWpQT","m2fe4910vhlj3cwsbw45wfc","tpmpx2xkasjhbntai7ejfx3","ydBpHsjLwIGYomDZo8NtB","g9m1mlmymq7qvcjzi8ap2qu"],"parent":null,"data":{},"body":"\n![home](/assets/images/home.png)\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.83.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"theme":"custom"},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"assetsPrefix":"/notes","copyAssets":true,"siteHierarchies":["root"],"enableSiteLastModified":true,"siteRootDir":"docs","siteUrl":"https://vinaykakkad.github.io","enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"writeStubs":false,"seo":{"title":"Notes","description":"Notes"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enablePrettyLinks":true,"enableTaskNotes":true,"theme":"custom","searchMode":"lookup","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}